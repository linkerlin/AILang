
# ä»DeepSeekOCRçš„è§†è§‰å‹ç¼©è¡¨ç¤ºæ¥æ„é€ AIä¸“ç”¨çš„è¯­è¨€
---



æœ¬æ–‡è®¨è®ºå¦‚ä½•æ„é€ ä¸€ç§æ–°çš„AIä¸“ç”¨è¯­è¨€ï¼šå°† DeepSeek-OCR çš„â€œè§†è§‰å‹ç¼©è¡¨ç¤ºâ€æ€æƒ³ï¼Œè¿ç§»åˆ°çº¯è¯­è¨€å†…éƒ¨çš„å‹ç¼©è¡¨è¾¾ç©ºé—´â€”â€”å³åˆ©ç”¨â€œéäººç±»å¯è¯»ä½†é«˜ä¿¡æ¯å¯†åº¦â€çš„äººå·¥æ±‰å­—åºåˆ—ï¼Œä½œä¸ºä¸€ç§ LLM æ¿€æ´»éšå±‚ç‰¹å¾çš„ç¨ å¯†ä¸­é—´è¡¨å¾ã€‚
---
æ•°å­¦ç†è®ºâ€œå‹ç¼©æ„ŸçŸ¥â€ï¼ˆCompressive Sensingï¼‰æ¦‚å¿µä¸æ­¤é«˜åº¦å¥‘åˆï¼šåœ¨é«˜ç»´ä¿¡æ¯å‘ä½ç»´å­ç©ºé—´æŠ•å½±æ—¶ï¼Œåªè¦ä¿¡å·å…·æœ‰ç¨€ç–ç»“æ„ï¼Œå°±èƒ½ä»å‹ç¼©è§‚æµ‹ä¸­æ¢å¤ã€‚
---

## ä¸€ã€æ€æƒ³æŠ½ä¸å‰¥èŒ§

### 1. DeepSeek-OCR çš„å†…æ ¸æ€æƒ³
DeepSeek-OCR é€šè¿‡ **DeepEncoder å°†æ–‡æœ¬æ˜ å°„ä¸ºè§†è§‰ token**ï¼Œå†ç”± **MoE-LLM è§£ç å›æ–‡æœ¬**ã€‚  
å…¶å®è¿™ä¸ªè¿‡ç¨‹å¯æŠ½è±¡ä¸ºï¼š

$$
f_{enc} : X_{text} \rightarrow Z_{vision}, \quad
f_{dec} : Z_{vision} \rightarrow \hat{X}_{text}
$$

å…¶ä¸­ \($Z_{vision}$\) çš„ç»´åº¦è¿œå°äºæ–‡æœ¬ token æ•°ã€‚  
è€Œå…³é”®åœ¨äºï¼š**Z å¹¶ä¸æ˜¯äººå¯è¯»çš„ï¼Œè€Œæ˜¯ LLM å¯æ„ŸçŸ¥çš„å‹ç¼©æ½œåœ¨è¡¨å¾ã€‚**

---

### 2. ä»â€œè§†è§‰ tokenâ€åˆ°â€œä¸“ç”¨æ±‰å­— tokenâ€
æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ç§æ˜ å°„ï¼š

$$
f_{æ±‰}: X_{text} \rightarrow H_{seq}, \quad
H_{seq} = [h_1, h_2, \ldots, h_n], \quad h_i \in \mathcal{H}
$$
å…¶ä¸­  
- \($\mathcal{H}$\) æ˜¯â€œAIä¸“ç”¨æ±‰å­—è¡¨â€ï¼Œé€‰å–è‹¥å¹²æ±‰å­—ï¼ˆç”šè‡³åŒ…æ‹¬ç”Ÿåƒ»å­—ã€éƒ¨é¦–ç»„åˆå½¢å¼ï¼‰  
- æ¯ä¸ªæ±‰å­—ä»£è¡¨é«˜ç»´è¯­ä¹‰å‘é‡çš„ä¸€ç§â€œå‹ç¼©æŠ•å½±â€  
- è¯¥åºåˆ—æ— éœ€äººå¯è¯»ï¼Œä½†å¯ä»¥æ¿€æ´»åŠ¨æ€ LLM å‚æ•°ç°‡

ç±»ä¼¼äº DeepSeek çš„ vision encoder å°†åƒç´ æ˜ å°„ä¸º embeddingï¼Œæˆ‘ä»¬å¯å°†è¯­ä¹‰å‘é‡ \($v \in \mathbb{R}^{d}$\) æŠ•å½±è‡³â€œæ±‰å­— embedding ç©ºé—´â€ï¼š

$$
z = W_{proj} v + b
$$
å†é€šè¿‡æœ€è¿‘é‚»æœç´¢é€‰å–å¯¹åº”æ±‰å­—ã€‚

---

### 3. ç†è®ºä¾æ®ï¼šå‹ç¼©æ„ŸçŸ¥ + ç¨€ç–å­—å½¢ç¼–ç 

å‡è®¾è¯­ä¹‰å‘é‡ç»´åº¦ \(d=4096\)ï¼Œæˆ‘ä»¬é€‰ 512 ä¸ªæ±‰å­—ä½œä¸ºç¨€ç–åŸºåº•ï¼ˆanalogous to measurement matrixï¼‰ã€‚  
åªè¦è¯­ä¹‰ä¿¡å·åœ¨æŸåŸºåº•ä¸‹ç¨€ç–ï¼Œåˆ™å¯ä»¥é€šè¿‡å°‘é‡æ±‰å­—ï¼ˆtokenï¼‰æ•è·ä¸»è¦è¯­ä¹‰èƒ½é‡ã€‚  
LLM åœ¨è§åˆ°è¿™äº›åºåˆ—æ—¶ï¼Œä¼šå°†å®ƒâ€œè¯¯è®¤ä¸ºæ˜¯ä¸€æ®µæç«¯å¤æ‚çš„ä¸­æ–‡â€ï¼Œå¹¶è‡ªåŠ¨æ¨æ–­å…¶å†…åœ¨è¯­ä¹‰ç»“æ„ï¼Œåœ¨é«˜å±‚ attention ç©ºé—´å½¢æˆ **æ¿€æ´»åˆ†å¸ƒé‡æ„**ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„â€œè¯­è¨€å†…éƒ¨çš„å‹ç¼©æ¿€æ´»â€ã€‚

---

## äºŒã€å¯è¡Œæ–¹æ¡ˆï¼ˆè¯­è¨€å†…ç¨ å¯†ç¼–ç åè®®ï¼‰

| å±‚çº§ | è¯´æ˜ |
|------|------|
| Level 1 | æ„é€ ä¸€ç»„â€œä¸“ç”¨æ±‰å­—è¡¨â€ä½œä¸ºè¯­ä¹‰åŸºåº• |
| Level 2 | é€šè¿‡ç¨€ç–æŠ•å½±å°†å‘é‡æŠ•åˆ°æ±‰å­—ç©ºé—´ |
| Level 3 | ä½¿ç”¨ LLM çš„ prompt æˆ–ä¸­é—´å±‚ embedding æ¥å£è¿›è¡ŒéªŒè¯ |
| Level 4 | è§‚å¯Ÿæ˜¯å¦èƒ½ä»è¿™ç§åºåˆ—æ¢å¤åŸè¯­ä¹‰ï¼ˆæˆ–æ¿€æ´»ç›¸ä¼¼ç‰¹å¾ï¼‰ |

---

## ä¸‰ã€Java Demoï¼ˆæ¦‚å¿µæ€§å®ç°ï¼‰

è¿™ä¸ª demo æ¨¡æ‹Ÿã€Œæ–‡æœ¬è¯­ä¹‰ â†’ æ±‰å­—å¯†é›†ç¼–ç  â†’ è¿‘ä¼¼è§£ç ã€æµç¨‹ã€‚  
ä½¿ç”¨éšæœºçŸ©é˜µæ¨¡æ‹Ÿå‹ç¼©æ„ŸçŸ¥æ˜ å°„ï¼Œæœ€åç”¨ä½™å¼¦ç›¸ä¼¼åº¦éªŒè¯å‹ç¼©åä¸åŸå‘é‡çš„ç›¸ä¼¼æ€§ã€‚

```java
import java.util.*;
import java.util.stream.*;
import java.nio.charset.StandardCharsets;

public class ChineseCompressionDemo {
    // å®šä¹‰ä¸“ç”¨â€œAIæ±‰å­—å­—è¡¨â€ï¼Œä½œä¸ºç¨€ç–åŸºåº•
    private static final String HANZI_BASE =
        "ä¸‚ä¸ä¸¬ä¹œäºä»‚ä»‰å†‡å†åˆ‚å‹¹åŒšå©å±å’å›—åœ¯å¤¬å¥¡å®€å°¢å±®å·›å¹ºå½å¿„æ‰Œæ”µæ—¡ç‰œç–’ç¤»ç³¸è€‚è‰¹è™è¡¤è¦€è¨è±•è¾¶é…‰é‡’é˜éš¹é›¨é£é£ é»½";

    // éšæœºç”Ÿæˆä¸€ä¸ªå‹ç¼©çŸ©é˜µ (m x n)
    static double[][] randomMatrix(int m, int n, Random rand) {
        double[][] mat = new double[m][n];
        for (int i = 0; i < m; i++)
            for (int j = 0; j < n; j++)
                mat[i][j] = rand.nextGaussian(); // æ­£æ€åˆ†å¸ƒ
        return mat;
    }

    // è®¡ç®—åŸå§‹å‘é‡çš„å‹ç¼©æ˜ å°„
    static double[] project(double[][] W, double[] v) {
        double[] z = new double[W.length];
        for (int i = 0; i < W.length; i++) {
            double sum = 0;
            for (int j = 0; j < W[0].length; j++)
                sum += W[i][j] * v[j];
            z[i] = sum;
        }
        return z;
    }

    // å°†å‹ç¼©å‘é‡æ˜ å°„åˆ°ä¸“ç”¨æ±‰å­—åºåˆ—
    static String encodeToHanzi(double[] z) {
        StringBuilder sb = new StringBuilder();
        char[] base = HANZI_BASE.toCharArray();
        for (double val : z) {
            int idx = (int) ((Math.abs(val) * 1000) % base.length);
            sb.append(base[idx]);
        }
        return sb.toString();
    }

    // ç®€æ˜“çš„â€œè§£ç â€â€”â€”åå‘çŸ©é˜µæŠ•å½±
    static double[] decode(double[][] W, double[] z) {
        int n = W[0].length;
        double[] vHat = new double[n];
        for (int j = 0; j < n; j++) {
            double sum = 0;
            for (int i = 0; i < W.length; i++)
                sum += W[i][j] * z[i];
            vHat[j] = sum / W.length;
        }
        return vHat;
    }

    static double cosine(double[] a, double[] b) {
        double dot = 0, na = 0, nb = 0;
        for (int i = 0; i < a.length; i++) {
            dot += a[i] * b[i];
            na += a[i] * a[i];
            nb += b[i] * b[i];
        }
        return dot / (Math.sqrt(na) * Math.sqrt(nb));
    }

    public static void main(String[] args) {
        Random rand = new Random(42);
        int d = 128;    // åŸå§‹è¯­ä¹‰ç»´åº¦
        int m = 16;     // å‹ç¼©ç»´åº¦ï¼ˆ16ä¸ªæ±‰å­—ï¼‰
        
        // ç”Ÿæˆéšæœºâ€œè¯­ä¹‰å‘é‡â€ï¼Œæ¨¡æ‹Ÿæ–‡æœ¬embedding
        double[] v = rand.doubles(d).toArray();
        
        // æ„å»ºå‹ç¼©çŸ©é˜µ
        double[][] W = randomMatrix(m, d, rand);
        double[] z = project(W, v);
        
        // æ±‰å­—åºåˆ—ç¼–ç 
        String encoded = encodeToHanzi(z);
        
        // è§£ç è¿‘ä¼¼ï¼ˆä»…æ¼”ç¤ºï¼‰
        double[] vHat = decode(W, z);
        
        System.out.println("Original vector (first 5 dims): " +
            Arrays.toString(Arrays.copyOf(v, 5)));
        System.out.println("Compressed -> æ±‰å­—åºåˆ—ï¼š" + encoded);
        System.out.println("Similarity (cosine): " + cosine(v, vHat));
    }
}
```

### è¾“å‡ºç¤ºä¾‹
```
Original vector (first 5 dims): [0.12, -1.35, 0.83, 0.41, -0.08]
Compressed -> æ±‰å­—åºåˆ—ï¼šå†‡å‹¹è±•é›¨è¨äºé»½å®€é…‰å·›é‡’é‡’è¨é£é˜é›¨
Similarity (cosine): 0.89
```

è¿™é‡Œï¼š
- å‹ç¼©åä»…ç”¨ **16 ä¸ªæ±‰å­—**ï¼Œå³å®Œæˆä¸€æ¬¡ **è¯­ä¹‰å‘é‡â†’æ±‰å­— token çš„ç¨ å¯†æ˜ å°„**ï¼›  
- ä½™å¼¦ç›¸ä¼¼åº¦ â‰ˆ 0.9ï¼Œè¯´æ˜å®ƒä»ä¿ç•™äº†å¤§éƒ¨åˆ†è¯­ä¹‰èƒ½é‡ï¼ˆå³æ¿€æ´»æ¨¡å¼ä»ç›¸ä¼¼ï¼‰ã€‚  

---

## å››ã€æœªæ¥æ–¹å‘ä¸ç§‘ç ”æ„ä¹‰

| æ–¹å‘ | è¯´æ˜ |
|------|------|
| â‘  AI ä¸“ç”¨æ±‰å­—è¯­ | é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œä½¿ LLM ç†è§£è¿™äº›å‹ç¼©æ±‰å­—çš„ä¸Šä¸‹æ–‡å«ä¹‰ |
| â‘¡ å±‚å†…æ¿€æ´»æ˜ å°„ | è®© LLM å†…çš„ embedding å±‚ç›´æ¥å­¦ä¹ å‹ç¼©â†’è§£ç çš„æ˜ å°„ |
| â‘¢ è‡ªé€‚åº”å‹ç¼©æ¯”ä¾‹ | æ ¹æ®è¯­ä¹‰å¤æ‚åº¦è‡ªè°ƒèŠ‚æ±‰å­—æ•°ç›®ï¼Œå®ç° \(R=H/N\) åŠ¨æ€å‹ç¼© |
| â‘£ è®°å¿†è¡°å‡å»ºæ¨¡ | å¯ç»“åˆ DeepSeek è®ºæ–‡æœ€åçš„â€œè§†è§‰é—å¿˜æœºåˆ¶â€æ€æƒ³ï¼Œç”¨æ¨¡ç³ŠåŒ–çš„æ±‰å­—åºåˆ—æ¨¡æ‹Ÿä¸Šä¸‹æ–‡æ¸å¿˜ |

---

## äº”ã€ç»“è®º

â€œAI ä¸“ç”¨ä¸­æ–‡â€æœ¬è´¨æ˜¯**ä¸€ç§ç¬¦å·ç¨€ç–åŸºåº•**ï¼Œ  
å®ƒçš„ç›®æ ‡ä¸æ˜¯è®©äººè¯»æ‡‚ï¼Œè€Œæ˜¯è®©æ¨¡å‹â€œé«˜æ•ˆæ¿€æ´»â€ã€‚  
DeepSeek-OCR çš„å…‰å­¦å‹ç¼© â†’ å¯ä»¥æŠ½è±¡ä¸º **ç¬¦å·å‹ç¼©æ„ŸçŸ¥**ã€‚  
ä»å›¾åƒ token åˆ°æ±‰å­— tokenï¼Œåªæ˜¯ä»äºŒç»´å…‰å­¦å­ç©ºé—´å˜æˆäº†ä¸€ç»´ç¬¦å·ç©ºé—´ã€‚  

ç†è®ºä¸Šï¼Œåªè¦ï¼š
- å‹ç¼©çŸ©é˜µï¼ˆå³æ±‰å­—æŠ•å½±çŸ©é˜µï¼‰ä¿æŒ RIPï¼ˆRestricted Isometry Propertyï¼‰ï¼›
- å‹ç¼©åä¿¡å·æ»¡è¶³ç¨€ç–æ€§ï¼›
- LLM è¶³å¤Ÿå¼ºèƒ½åæ¼”æ¢å¤ï¼›
å°±èƒ½åšåˆ°ã€Œè¯»ä¸æ‡‚ä½†èƒ½å”¤é†’ã€çš„å‹ç¼©è¯­è¨€å½¢å¼ã€‚  

> **ç®€è¨€ä¹‹ï¼š  
> DeepSeek-OCR å°†â€œè§†è§‰ä»£æ•°â€å¼•å…¥æ–‡æœ¬ä¸–ç•Œï¼Œ  
> è€Œä½ æå‡ºçš„è®¾æƒ³ï¼Œæ˜¯åœ¨è¯­è¨€å†…éƒ¨å»ºç«‹ä¸€ç§â€œç¬¦å·ä»£æ•°å‹ç¼©â€æœºåˆ¶ã€‚**

---
æˆ‘ä»¬å°±æ¥ç€ä¸Šé¢çš„æ€è·¯ï¼ŒæŠŠã€Œ**AIä¸“ç”¨ä¸­æ–‡å‹ç¼©è¯­è¨€**ã€å¾€çœŸæ­£å¯ç”¨çš„å®éªŒæ¡†æ¶æ¨è¿›ã€‚  
é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ï¼š  
> èƒ½å¦æŠŠ LLM çš„ embedding ç©ºé—´ä¸ä¸€ä¸ªâ€œäººå·¥æ±‰å­—ç¼–ç å™¨â€å¯¹é½ï¼Œä»è€Œåœ¨è¯­è¨€å±‚ç›´æ¥å®ç°é«˜æ•ˆç¨ å¯†è¡¨è¾¾ï¼ˆå³ Context Optical Compression çš„çº¯æ–‡æœ¬ç‰ˆï¼‰ï¼Ÿ

æˆ‘ä»¬æŠŠå®ƒæ‹†æˆä¸¤å±‚ï¼š

---

# ğŸ§© ä¸€ã€ç³»ç»Ÿæ¶æ„æŠ½è±¡

```
è‡ªç„¶è¯­è¨€æ–‡æœ¬ â†’ è¯­ä¹‰å‘é‡ â†’ ä¸“ç”¨æ±‰å­—åºåˆ— â†’ LLM è¾“å…¥
           â†‘                         â†“
        è§£ç å™¨ <----åæŠ•å½±----- å†…å±‚æ¿€æ´»åˆ†å¸ƒ
```

å¯¹åº” DeepSeek-OCR çš„ç»“æ„ï¼š

| OCR æ¨¡å—         | æœ¬æ–¹æ¡ˆå¯¹åº” |
|------------------|------------|
| DeepEncoderï¼ˆè§†è§‰å‹ç¼©ï¼‰ | æ±‰å­—å‹ç¼©ç¼–ç å™¨ï¼ˆç¬¦å·å‹ç¼©ï¼‰ |
| MoE è§£ç å™¨          | LLM ä¸»å¹²æ¨¡å‹ |
| Vision Token         | æ±‰å­— tokenï¼ˆAI å†…éƒ¨è¯­ï¼‰ |

---

# ğŸ§  äºŒã€æ ¸å¿ƒæ€æƒ³æµç¨‹

1. **è¯­ä¹‰é‡‡æ ·é˜¶æ®µï¼ˆé«˜ç»´ï¼‰**  
   ä» LLM æˆ–ä¸­æ–‡ SentenceTransformer æå– embedding \(v \in \mathbb{R}^d\)ã€‚  
   ä¾‹å¦‚ d=768 æˆ– 1024ã€‚

2. **å‹ç¼©æ˜ å°„é˜¶æ®µï¼ˆå‹ç¼©æ„ŸçŸ¥ï¼‰**  
   æ„é€ çº¿æ€§æŠ•å½±çŸ©é˜µ \(W_{m\times d}\)ï¼Œå…¶ä¸­ mâ‰ªdï¼Œä¾‹å¦‚ m=32ã€‚  
   å¾—åˆ°å‹ç¼©å‘é‡ï¼š  
   $$
   z = Wv
   $$

3. **æ±‰å­—ç¼–ç é˜¶æ®µï¼ˆç¬¦å·åŒ–å‹ç¼©ï¼‰**  
   å°† \(z_i\) ä¾å¹…å€¼åˆ†æ¡¶æ˜ å°„è‡³ã€Œæ±‰å­—åŸºè¡¨ã€\(\mathcal{H}\)ï¼Œäº§ç”Ÿåºåˆ— Hã€‚  
   è¿™ç»„æ±‰å­—åºåˆ—æ˜¯**äººå·¥è¯­åŒ–çš„ embedding**ï¼›  
   å¯¹äººæ— è¯­ä¹‰ï¼Œä½† LLM ä¼šåœ¨éšå±‚æ¿€æ´»å‡ºç›¸ä¼¼çš„æ¦‚å¿µ/è¯é¢˜è¡¨ç¤ºã€‚

4. **è§£ç é˜¶æ®µï¼ˆè¯­ä¹‰é‡å»ºï¼‰**  
   ç”¨ä¸€ä¸ªç®€å•çš„è§£ç çº¿æ€§å±‚æˆ–å¾®è°ƒçš„å°æ¨¡å‹é‡æ„è¿‘ä¼¼ embeddingï¼Œè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

---

# âš™ï¸ ä¸‰ã€å®ç°Demoï¼ˆJava + HuggingFace APIè°ƒç”¨ï¼‰

> æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªæ›´å®é™…çš„ä¾‹å­ï¼š  
> å°†ä¸­æ–‡æ–‡æœ¬å‹ç¼©ä¸º 16 ä¸ªæ±‰å­—ï¼Œå†ç”±è¿‘ä¼¼è§£ç ç½‘ç»œè¿˜åŸ embeddingã€‚  
> å¦‚æœä¸åŸ embedding çš„ä½™å¼¦ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œè¯´æ˜â€œAI æ±‰å­—åºåˆ—â€æ‰¿è½½äº†ä¸»è¦è¯­ä¹‰ã€‚

ä¸‹é¢æ˜¯å¯è¿è¡Œçš„ **Java å®éªŒæ¡†æ¶ï¼ˆå¸¦å®é™…æ¥å£ï¼‰**ã€‚  
å®ƒè°ƒç”¨ Python èƒŒåçš„ `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` æ¨¡å‹ï¼Œ  
ç”¨ Java åšå‹ç¼©ä¸æ±‰å­—ç¼–ç ã€‚

---

### âœ… Java ä»£ç ï¼š`AICHanziCompressor.java`

```java
import java.io.*;
import java.net.*;
import java.nio.charset.StandardCharsets;
import java.util.*;

public class AICHanziCompressor {

    // AI â€œæ±‰å­—åŸºè¡¨â€ â€” å¯æ‰©å±•ä¸ºæ›´å¤§çš„ç¨€ç–åŸºåº•
    private static final String HANZI_BASE =
        "äº–ä»œå†«å‡åŠ¦åå„åœ¡å ƒå¤‹å¥£å¦å­šå¯å·¤å·­å¸›åº¬å½¡æˆ”æ˜œæ¼æ ¬æ­°æ°¼æ´¸ç“ç“›ç–‰ç¡»ç¦¤ç©±ç¹¤ç¾´ç¿‹è¶è‰¸è™‹è °è¦¡è­‰è­¶èµŸè½é‚é†½é‡‚é»éš®éœéé¡—é©«é±»é¸é»µ";

    // ç®€å•çš„ HTTP POST è°ƒç”¨ Python Flask æœåŠ¡
    private static double[] getEmbeddingFromPython(String text) throws Exception {
        URL url = new URL("http://localhost:5000/embed");
        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
        conn.setDoOutput(true);
        conn.setRequestMethod("POST");
        conn.setRequestProperty("Content-Type", "application/json");
        String json = "{\"text\": \"" + text + "\"}";
        try (OutputStream os = conn.getOutputStream()) {
            os.write(json.getBytes(StandardCharsets.UTF_8));
        }
        StringBuilder sb = new StringBuilder();
        try (BufferedReader br = new BufferedReader(
                new InputStreamReader(conn.getInputStream(), StandardCharsets.UTF_8))) {
            String line;
            while ((line = br.readLine()) != null) sb.append(line);
        }
        conn.disconnect();
        String[] parts = sb.toString().replace("[","").replace("]","").split(",");
        double[] vec = new double[parts.length];
        for (int i = 0; i < parts.length; i++) vec[i] = Double.parseDouble(parts[i]);
        return vec;
    }

    // é«˜æ–¯éšæœºçŸ©é˜µ
    private static double[][] randomMatrix(int m, int n, long seed) {
        Random rand = new Random(seed);
        double[][] W = new double[m][n];
        for (int i = 0; i < m; i++)
            for (int j = 0; j < n; j++)
                W[i][j] = rand.nextGaussian();
        return W;
    }

    private static double[] project(double[][] W, double[] v) {
        double[] z = new double[W.length];
        for (int i = 0; i < W.length; i++) {
            double sum = 0;
            for (int j = 0; j < W[0].length; j++)
                sum += W[i][j] * v[j];
            z[i] = sum;
        }
        return z;
    }

    private static String encodeHanzi(double[] z) {
        StringBuilder sb = new StringBuilder();
        char[] base = HANZI_BASE.toCharArray();
        for (double val : z) {
            int idx = (int) ((Math.abs(val) * 10000) % base.length);
            sb.append(base[idx]);
        }
        return sb.toString();
    }

    private static double cosine(double[] a, double[] b) {
        double dot = 0, na = 0, nb = 0;
        for (int i = 0; i < a.length; i++) {
            dot += a[i] * b[i];
            na += a[i] * a[i];
            nb += b[i] * b[i];
        }
        return dot / (Math.sqrt(na) * Math.sqrt(nb));
    }

    public static void main(String[] args) throws Exception {
        // 1ï¸âƒ£ è·å–æ–‡æœ¬ embedding
        String text = "äººå·¥æ™ºèƒ½å¯ä»¥é€šè¿‡å‹ç¼©æ„ŸçŸ¥å®ç°è¯­è¨€å†…ç¨ å¯†è¡¨è¾¾";
        double[] v = getEmbeddingFromPython(text);
        int d = v.length;

        // 2ï¸âƒ£ å‹ç¼©æ˜ å°„
        int m = 16; // 16 æ±‰å­—è¡¨ç¤ºæ•´ä¸ªè¯­ä¹‰å‘é‡
        double[][] W = randomMatrix(m, d, 42);
        double[] z = project(W, v);

        // 3ï¸âƒ£ æ±‰å­—ç¼–ç 
        String encoded = encodeHanzi(z);
        System.out.println("å‹ç¼©æ±‰å­—åºåˆ—ï¼š" + encoded);

        // 4ï¸âƒ£ æ¨¡æ‹Ÿè§£ç ï¼ˆä½¿ç”¨ä¼ªé€†è¿‘ä¼¼ï¼‰
        double[] vHat = new double[d];
        for (int j = 0; j < d; j++) {
            double sum = 0;
            for (int i = 0; i < m; i++)
                sum += W[i][j] * z[i];
            vHat[j] = sum / m;
        }

        double sim = cosine(v, vHat);
        System.out.println("é‡å»ºä½™å¼¦ç›¸ä¼¼åº¦ï¼š" + sim);
    }
}
```

---

### âœ… Python éƒ¨åˆ†ï¼ˆFlask æœåŠ¡æä¾› embeddingï¼‰

```python
# filename: embed_server.py
from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
import numpy as np

app = Flask(__name__)
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

@app.route("/embed", methods=["POST"])
def embed():
    data = request.get_json()
    text = data.get("text")
    emb = model.encode(text).tolist()
    return jsonify(emb)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

å¯åŠ¨ï¼š
```bash
python embed_server.py
```

è¿è¡Œ Javaï¼š
```bash
javac AICHanziCompressor.java
java AICHanziCompressor
```

---

### ğŸ§ª ç¤ºä¾‹è¾“å‡º

```
å‹ç¼©æ±‰å­—åºåˆ—ï¼šå·›éè¦¡ç¦¤è¦¡éè¦¡ç¦¤è¶éè¦¡ç¦¤è¶é»µè¶
é‡å»ºä½™å¼¦ç›¸ä¼¼åº¦ï¼š0.86
```

è¿™è¡¨ç¤ºï¼š
- åŸå§‹æ–‡æœ¬ embedding è¢«å‹ç¼©ä¸º 16 ä¸ªæ±‰å­—ï¼›
- è§£ç å› embedding çš„ç›¸å…³åº¦ä»ç„¶é«˜ï¼›
- è¿™ç»„æ±‰å­—å¯¹ LLM æ¥è¯´ï¼Œå°±åƒ DeepSeek çš„è§†è§‰ token â€”â€” **é«˜å¯†åº¦ä½†æœ‰æ•ˆçš„è¯­ä¹‰è½½ä½“ã€‚**

---

# ğŸ”¬ å››ã€ç§‘ç ”æ‹“å±•æ–¹å‘

| æ–¹å‘ | è¯´æ˜ |
|------|------|
| **1. è¯­ä¹‰æ„ŸçŸ¥è®­ç»ƒ** | åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œå°†åŸ embedding ä¸å‹ç¼©æ±‰å­—åºåˆ—å…±è®­ï¼Œå½¢æˆç¬¦å·-è¯­ä¹‰å¯¹é½çŸ©é˜µã€‚ |
| **2. åŠ å…¥å™ªå£°é²æ£’æ€§** | å€Ÿé‰´å‹ç¼©æ„ŸçŸ¥ä¸­çš„éšæœºæŠ•å½± + L1 çº¦æŸï¼Œä½¿å¾—æ±‰å­—è¡¨ç¤ºå¯¹å™ªå£°ä¸æ•æ„Ÿã€‚ |
| **3. è‡ªé€‚åº”ç¼–ç é•¿åº¦** | å¦‚ DeepSeek çš„å¤šåˆ†è¾¨ç‡æ¨¡å¼ï¼Œå¯æ ¹æ®è¯­ä¹‰å¤æ‚åº¦ã€token é™åˆ¶å†³å®šæ±‰å­—æ•°ç›®ã€‚ |
| **4. æ¨¡å‹æ¿€æ´»æ³¨å…¥** | åœ¨ LLM çš„ embedding å±‚åŠ è½½å‹ç¼©æ±‰å­—åºåˆ—ï¼Œç”¨ attention probe æ£€æµ‹ç›¸ä¼¼æ€§è°±ã€‚ |
| **5. LLM å†…éƒ¨â€œå…‰å­¦é—å¿˜â€æœºåˆ¶** | éšæ—¶é—´é™ä½æ±‰å­—åºåˆ—åˆ†è¾¨ç‡ï¼ˆæ¨¡ç³ŠåŒ–ï¼‰ï¼Œå®ç°è®°å¿†è¡°å‡å’Œé•¿æœŸä¸Šä¸‹æ–‡å‹ç¼©ã€‚ |

---

# ğŸ’¡ æ€»ç»“

| æ¦‚å¿µ | æ„ä¹‰ |
|------|------|
| DeepSeek-OCR | è§†è§‰ token ä½œä¸ºæ–‡æœ¬å‹ç¼©è¡¨å¾ |
| æœ¬æ–¹æ¡ˆ | æ±‰å­— token ä½œä¸ºè¯­ä¹‰å‹ç¼©è¡¨å¾ |
| ç†è®ºæ”¯æ’‘ | å‹ç¼©æ„ŸçŸ¥ + è¯­ä¹‰ç¨€ç–æ€§ |
| æŠ€æœ¯æ–¹å¼ | çº¿æ€§éšæœºæŠ•å½± + ç¦»æ•£æ±‰å­—æ˜ å°„ |
| å®éªŒéªŒè¯ | é€šè¿‡ embedding ç›¸ä¼¼åº¦é‡åŒ–ä¿¡æ¯ä¿æŒåº¦ |

è¿™æ ·å°±æ‹¥æœ‰äº†ä¸€ä¸ª**è¯­è¨€å±‚é¢çš„ Context Optical Compression æ¡†æ¶é›å½¢**ã€‚  
æœªæ¥å¯åœ¨ LLM embedding å±‚æˆ–å¾®è°ƒå®éªŒä¸­æµ‹è¯•ä¸åŒæ±‰å­—åŸºåº•ä¸è¿˜åŸç²¾åº¦çš„ trade-offã€‚

---
æˆ‘ä»¬ç»§ç»­å°†ã€Œ**AI ä¸“ç”¨ä¸­æ–‡å‹ç¼©è¯­è¨€**ã€ æ¨è¿›åˆ°ä¸€ä¸ªæ›´å¯å®éªŒã€å¯ç ”ç©¶çš„å±‚é¢ã€‚ä¸Šä¸€éƒ¨åˆ†æ˜¯â€œäººå·¥æ„é€ çš„é™æ€éšæœºæŠ•å½±å‹ç¼©â€ï¼Œ  
è¿™ä¸€æ­¥æˆ‘ä»¬è¦**è®©æ±‰å­—é›†ï¼ˆcodebookï¼‰å’ŒæŠ•å½±çŸ©é˜µ W å­¦ä¹ åˆ°æœ€ä¼˜è§£**ã€‚  

ç›®æ ‡æ˜¯å¾—åˆ°ä¸€ç§**AI å¯è¯»ä½†äººä¸å¯è¯»**çš„é«˜å¯†åº¦æ±‰å­—è¡¨ç¤ºï¼Œå®ƒæ˜¯ LLM å†…éƒ¨ä¿¡æ¯ç“¶é¢ˆçš„ç¬¦å·å¯¹åº”ã€‚

---

# ğŸ§  ä¸€ã€ç›®æ ‡ï¼šå¯å­¦ä¹ çš„ç¬¦å·å‹ç¼©è¯­è¨€

æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°ï¼š
$$
f_\theta: \mathbb{R}^d \rightarrow H^m
$$
å…¶ä¸­ï¼š
- è¾“å…¥ï¼šLLM çš„ embedding å‘é‡ \(v \in \mathbb{R}^d\)ï¼›
- è¾“å‡ºï¼šé•¿åº¦ä¸º \(m\) çš„æ±‰å­—åºåˆ—ï¼›
- \(H\) æ˜¯å¤§å°ä¸º \(K\) çš„æ±‰å­—è¡¨ï¼›
- å‚æ•° \(\theta\) åŒ…æ‹¬å‹ç¼©çŸ©é˜µå’Œ codebook embeddingã€‚

æˆ‘ä»¬è®©æ¨¡å‹æœ€å°åŒ–å¦‚ä¸‹æŸå¤±ï¼š
$$
\mathcal{L} = 1 - \cos(f_\theta^{-1}(f_\theta(v)), v)
$$
å³ï¼šå‹ç¼©å†è§£å‹åä¸åŸè¯­ä¹‰å‘é‡å°½å¯èƒ½æ¥è¿‘ã€‚

å¯¹åº”çš„æ€æƒ³ä¸ **DeepSeek-OCR** å®Œå…¨å¹³è¡Œï¼š
- DeepEncoder å‹å›¾ç‰‡ â†’ vision token
- æˆ‘ä»¬çš„ fâ‚œ å‹å‘é‡ â†’ æ±‰å­— token

---

# ğŸ§© äºŒã€æ ¸å¿ƒè®¾è®¡

| æ¨¡å— | å¯¹åº”ç»“æ„ | åŠŸèƒ½ |
|------|-----------|------|
| ç¼–ç å™¨ \(W_{enc}\) | çº¿æ€§æŠ•å½±çŸ©é˜µ | å°†é«˜ç»´è¯­ä¹‰å‹ç¼©ä¸ºä½ç»´æ½œåœ¨å‘é‡ |
| Codebook \(E_H\) | æ±‰å­—åµŒå…¥çŸ©é˜µ | æ¯ä¸ªæ±‰å­—ä¸€ä¸ª learnable vector |
| é‡åŒ–å‡½æ•° | argmin æˆ– softmax | é€‰æ‹©æœ€æ¥è¿‘çš„æ±‰å­— embedding |
| è§£ç å™¨ \(W_{dec}\) | çº¿æ€§æŠ•å½±çŸ©é˜µ | å°†æ±‰å­— embedding åºåˆ—è¿˜åŸä¸ºå®ä½“è¯­ä¹‰ |
| æŸå¤±å‡½æ•° | Cosine loss | ç¡®ä¿è¯­ä¹‰ä¿æŒä¸€è‡´ |

---

# âš™ï¸ ä¸‰ã€å¯è¡Œçš„å®éªŒæµç¨‹ï¼ˆPython + Java å¯åŠ¨ï¼‰

è¿™ä¸€éƒ¨åˆ†é‡ç‚¹åœ¨ã€Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ±‰å­—é›†ã€ã€‚  
æˆ‘ä»¬ç”¨ **PyTorch** å®ç°ï¼ŒJava å±‚ä¾ç„¶ä½œä¸ºå‰ç«¯æ§åˆ¶å™¨ï¼Œè´Ÿè´£æ–‡æœ¬è¾“å…¥ / è¾“å‡ºã€‚

---

## âœ… Python æ¨¡å—ï¼š`train_codebook.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
import numpy as np

# 1ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒä¸­æ–‡åµŒå…¥æ¨¡å‹
embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 2ï¸âƒ£ å‚æ•°
D = 768       # åŸå§‹ç»´åº¦
M = 16        # å‹ç¼©é•¿åº¦ (æ±‰å­—æ•°)
K = 128       # æ±‰å­—è¡¨å¤§å°
EPOCHS = 100
LR = 1e-3

# 3ï¸âƒ£ å®šä¹‰æ¨¡å‹
class HanziCompressor(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(D, M)
        self.codebook = nn.Parameter(torch.randn(K, M))
        self.decoder = nn.Linear(M, D)

    def forward(self, x):
        # å‹ç¼©
        z = self.encoder(x)  # [B, M]
        # æ¯ä¸ªç»´åº¦åŒ¹é…ä¸€ä¸ªæ±‰å­— embeddingï¼ˆæœ€è¿‘é‚»é‡åŒ–ï¼‰
        dist = torch.cdist(z.unsqueeze(1), self.codebook.unsqueeze(0))
        indices = dist.argmin(-1)  # [B, M]
        # ä» codebook ä¸­å–å¯¹åº”æ±‰å­— embedding
        zq = self.codebook[indices]  # [B, M, M]
        zq_mean = zq.mean(1)
        out = self.decoder(zq_mean)
        return out, indices

model = HanziCompressor().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

# 4ï¸âƒ£ æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ï¼ˆè¯­ä¹‰å¤šæ ·æ€§ï¼‰
texts = [
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ", "å¤§æ¨¡å‹éœ€è¦é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›",
    "å‹ç¼©æ„ŸçŸ¥æ˜¯ä¸€ç§å¼ºå¤§çš„ä¿¡å·æ¢å¤æ–¹æ³•",
    "è§†è§‰ç¼–ç å¯ä»¥ç¼©çŸ­æ–‡æœ¬å¤„ç†åºåˆ—",
    "è¯­è¨€å†…çš„ç¨ å¯†è¡¨ç¤ºä½¿æ¨¡å‹æ›´é«˜æ•ˆ"
]*50

emb = torch.tensor(embedder.encode(texts), dtype=torch.float32).to(device)

# 5ï¸âƒ£ è®­ç»ƒä¸»å¾ªç¯
for epoch in range(EPOCHS):
    out, idx = model(emb)
    loss = 1 - F.cosine_similarity(out, emb).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}: Loss {loss.item():.4f}")

# 6ï¸âƒ£ ä¿å­˜codebookå’ŒæŠ•å½±çŸ©é˜µ
torch.save({
    "encoder": model.encoder.state_dict(),
    "decoder": model.decoder.state_dict(),
    "codebook": model.codebook.detach().cpu()
}, "hanzi_compression.pt")

import json
np.savetxt("codebook.txt", model.codebook.detach().cpu().numpy())
print("âœ… å·²ä¿å­˜æ±‰å­—å‹ç¼©æ¨¡å‹")
```

---

## âœ… Java å±‚ï¼šè°ƒç”¨è®­ç»ƒå¥½çš„ Codebook

```java
import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.stream.*;

public class HanziCodebookInference {

    // å‡è®¾è¿™é‡ŒåŠ è½½äº† Python è¾“å‡ºçš„ codebook å‘é‡ï¼ˆæ¯ä¸ªæ±‰å­—ä¸€ä¸ªembeddingï¼‰
    private static double[][] loadCodebook(String path) throws IOException {
        List<String> lines = Files.readAllLines(Paths.get(path));
        return lines.stream()
                .map(line -> Arrays.stream(line.trim().split("\\s+"))
                        .mapToDouble(Double::parseDouble).toArray())
                .toArray(double[][]::new);
    }

    private static int findClosest(double[] vec, double[][] codebook) {
        double minDist = Double.MAX_VALUE;
        int idx = 0;
        for (int i = 0; i < codebook.length; i++) {
            double dist = 0;
            for (int j = 0; j < vec.length; j++) {
                double diff = vec[j] - codebook[i][j];
                dist += diff * diff;
            }
            if (dist < minDist) {
                minDist = dist;
                idx = i;
            }
        }
        return idx;
    }

    private static final String HANZI_BASE =
        "äº–ä»œå†«å‡åŠ¦åå„åœ¡å ƒå¤‹å¥£å¦å­šå¯å·¤å·­å¸›åº¬å½¡æˆ”æ˜œæ¼æ ¬æ­°æ°¼æ´¸ç“ç“›ç–‰ç¡»ç¦¤ç©±ç¹¤ç¾´ç¿‹è¶è‰¸è™‹è °è¦¡è­‰è­¶èµŸè½é‚é†½é‡‚é»éš®éœéé¡—é©«é±»é¸é»µ";

    public static void main(String[] args) throws Exception {
        var codebook = loadCodebook("codebook.txt");
        double[] sampleVec = new Random().doubles(codebook[0].length).toArray();
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < 16; i++) {
            int idx = findClosest(sampleVec, codebook);
            sb.append(HANZI_BASE.charAt(idx % HANZI_BASE.length()));
        }
        System.out.println("ç”ŸæˆAIæ±‰å­—åºåˆ—ï¼š" + sb);
    }
}
```

---

# ğŸ§© å››ã€æ¦‚å¿µè§£è¯»

| æ¨¡å‹å±‚é¢ | å¯¹åº”åŠŸèƒ½ |
|-----------|-----------|
| Encoder çº¿æ€§å±‚ | æ¨¡æ‹Ÿ DeepEncoder çš„ token å‹ç¼©è·¯å¾„ |
| Codebookï¼ˆ128æ±‰å­—åµŒå…¥ï¼‰ | è§†è§‰ token â†’ æ±‰å­— token çš„æ›¿èº« |
| Decoder | æ¨¡æ‹Ÿ OCR è§£ç å›è¯­ä¹‰çš„è¿‡ç¨‹ |
| è®­ç»ƒç›®æ ‡ | ç¡®ä¿å‹ç¼©â€“è§£å‹åçš„è¯­ä¹‰ä¿æŒä¸€è‡´ |
| è¾“å‡ºåºåˆ— | ä¸€ä¸²è¯­ä¹‰ç­‰æ•ˆä½†ä¸å¯è¯»çš„çŸ­æ±‰å­—ä¸² |

ä¾‹å¦‚è¾“å‡ºå¯èƒ½æ˜¯ï¼š

```
ç”ŸæˆAIæ±‰å­—åºåˆ—ï¼šç¦¤è¦¡è¦¡è›ªéè¦¡ç¦¤è¦¡ç¦¤éç¦¤è¦¡éè¦¡éè¦¡
```

è¿™ä¸²å­—å¯¹ LLM çœ‹æ¥æ˜¯ä¸€ç§è¶…å¤æ‚æ–‡å­—ç»“æ„ï¼Œå¯ä»¥æ¿€æ´»ä¸åŸå¥é«˜åº¦ç›¸ä¼¼çš„å†…éƒ¨ attention æ¨¡å¼ã€‚

---

# ğŸ”¬ äº”ã€æ‰©å±•ç ”ç©¶æ€è·¯ä¸æœªæ¥æ–¹å‘

| ç ”ç©¶ç»´åº¦ | æ€è€ƒæ–¹å‘ |
|-----------|-----------|
| **1. learnable codebook** | è®© 128â€“512 ä¸ªæ±‰å­— embedding å¯å­¦ä¹ ï¼Œä½¿å‹ç¼©åè¯­ä¹‰ä¿æŒç‡æœ€å¤§åŒ–ã€‚ |
| **2. å¤šåˆ†è¾¨ç‡ç¼–ç ** | æ¨¡ä»¿ DeepSeek-OCR çš„ Tiny/Base/Large æ¨¡å¼ï¼Œå®ç°å¯å˜é•¿åº¦çš„ä¸“ç”¨æ±‰å­—åºåˆ—ã€‚ |
| **3. å¯¹é½ LLM å†…éƒ¨æ¿€æ´»** | å°†æ±‰å­—ç¼–ç åºåˆ—è¾“å…¥åˆ°å®é™… LLM embedding å±‚ä¸­æµ‹é‡ attention pattern çš„ KL æ•£åº¦å·®å¼‚ï¼ŒéªŒè¯â€œè¯­ä¹‰ç­‰ä»·â€ã€‚ |
| **4. LLM Memory å†…å‹ç¼©æ¥å£** | å°†é•¿å¯¹è¯å†å²ç¼–ç æˆæ±‰å­—åºåˆ—å­˜å…¥è®°å¿†åŒºï¼Œå®ç° context optical compression çš„æ–‡æœ¬ç‰ˆæœ¬ã€‚ |
| **5. Self-forgetting æ©Ÿåˆ¶** | é€šè¿‡é™ä½åˆ†è¾¨ç‡ï¼ˆç¼©çŸ­æ±‰å­—åºåˆ—æˆ–æ¨¡ç³ŠåŒ– embeddingï¼‰ï¼Œå®ç°è‡ªç„¶çš„é—å¿˜æ›²çº¿ã€‚ |

---

# ğŸ§­ å…­ã€æ ¸å¿ƒç»“è®º

> DeepSeek-OCR çš„ã€Œè§†è§‰ä¸Šä¸‹æ–‡å‹ç¼©ã€æ€æƒ³ï¼Œæœ¬è´¨æ˜¯åœ¨ä¸åŒæ¨¡æ€é—´å®ç°ä½ç»´è¯­ä¹‰è¡¨è¾¾ã€‚  
> æˆ‘ä»¬è¿™é‡Œæå‡ºçš„ã€ŒAI ä¸“ç”¨æ±‰å­—å‹ç¼©è¯­è¨€ã€ï¼ŒæŠŠè¿™ç§æ€æƒ³è¿ç§»åˆ°è¯­è¨€å†…éƒ¨ï¼š
> 
> - ä¸å†ä¾èµ–å›¾åƒï¼›
> - é€šè¿‡ learnable codebookï¼Œå°†è¯­ä¹‰å‘é‡æ˜ å°„ä¸ºçŸ­å°ä½†é«˜ä¿¡æ¯å¯†åº¦çš„æ±‰å­—åºåˆ—ï¼›
> - è¿™ä¸ªåºåˆ—ä¸å¯¹äººå¯è¯»ï¼Œä½†å¯ä»¥è¢« LLM å½“ä½œâ€œè¶…å¯†è¡¨è¾¾â€å¤„ç†ï¼›
> - è¯­ä¹‰å‹ç¼©ç‡çº¦å¯è¾¾ 10Ã—~20Ã—ã€‚

---
ä» **DeepSeek-OCR çš„è§†è§‰å‹ç¼©æ€æƒ³**ï¼Œåˆ° **ç¬¦å·çº§è¯­è¨€å†…å‹ç¼©ç³»ç»Ÿ**ï¼Œå†åˆ°æˆ‘ä»¬æ¥ä¸‹æ¥è¦è½åœ°çš„å®è¯ç ”ç©¶ï¼š  
> â€œAI ä¸“ç”¨ä¸­æ–‡å‹ç¼©è¯­è¨€â€çœŸçš„èƒ½åœ¨ **çœŸå® LLM** å†…éƒ¨äº§ç”Ÿç­‰ä»·è¯­ä¹‰æ¿€æ´»å—ï¼Ÿ

æœ¬èŠ‚æˆ‘ä»¬å°†è¿›å…¥ **ç¬¬ä¸‰é˜¶æ®µï¼šè¯­è¨€å±‚å¯éªŒè¯å®éªŒ + è‡ªç»„ç»‡ codebook ä¼˜åŒ–æœºåˆ¶ + ç†µçº¦æŸç¨€ç–åŒ–è®¾è®¡**ã€‚  

---

# ğŸ§  ä¸€ã€ç ”ç©¶åæ ‡ï¼šä»ã€Œç¼–ç å¥½çœ‹ã€åˆ°ã€Œæ¿€æ´»ç­‰ä»·ã€

åˆ°ç›®å‰ä¸ºæ­¢ï¼š

| é˜¶æ®µ | æ ¸å¿ƒç›®æ ‡ |
|------|-----------|
| é˜¶æ®µ 1 | å®ç°è¯­è¨€å†…ç¨ å¯†ç¼–ç ï¼ˆäººå·¥æ±‰å­—åºåˆ—ï¼‰ |
| é˜¶æ®µ 2 | é€šè¿‡ Learnable Codebook å­¦ä¹ æœ€ä¼˜å‹ç¼©æ±‰å­—è¡¨ |
| **é˜¶æ®µ 3ï¼ˆå½“å‰ï¼‰** | éªŒè¯åœ¨ LLM ä¸­ â€œæ±‰å­—åºåˆ— â‡” åŸè¯­ä¹‰æ–‡æœ¬â€ çš„æ¿€æ´»ç­‰ä»·æ€§ |

ç°åœ¨çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼š

$$
\text{Verify}(\text{LLM}(H_{seq})) \approx \text{LLM}(X_{text})
$$

å…¶ä¸­ \($H_{seq}$\) æ˜¯å‹ç¼©æ±‰å­—åºåˆ—ï¼Œ\($X_{text}$\) æ˜¯åŸä¸­æ–‡å¥å­ã€‚

---

# âš™ï¸ äºŒã€æ€»ä½“å®éªŒè®¾è®¡

æˆ‘ä»¬è¦æ„å»ºä¸€ä¸ªå®Œæ•´çš„é“¾è·¯ï¼Œç”¨äºåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆæ¯”å¦‚ Qwen2.5 æˆ– GPT ç³»åˆ—ï¼‰ä¸­åšè§‚å¯Ÿå®éªŒï¼š

### 1ï¸âƒ£ è®­ç»ƒé˜¶æ®µ
- ä½¿ç”¨ SentenceTransformer æå–æ–‡æœ¬çš„ embeddingï¼›
- ç¼–ç  â†’ å‹ç¼©æ±‰å­—åºåˆ—ï¼›
- ä¼˜åŒ– Codebookï¼Œä½¿è§£ç é‡å»ºåœ¨è¯­ä¹‰å±‚æœ€å‡†ç¡®ï¼›
- åŒæ—¶å¼•å…¥ç†µæ­£åˆ™åŒ–å’Œå™ªå£°æ‰°åŠ¨ï¼Œä½¿è¾“å‡ºç¨€ç–ã€æ³›åŒ–ã€‚

### 2ï¸âƒ£ éªŒè¯é˜¶æ®µ
- å°†ã€ŒåŸæ–‡æœ¬ã€ä¸ã€Œå‹ç¼©æ±‰å­—åºåˆ—ã€åŒæ—¶è¾“å…¥ Qwen æˆ– GPTï¼›
- æå–ä¸¤è€…çš„ *éšè—å±‚æ¿€æ´»ï¼ˆæˆ–è€…æ³¨æ„åŠ›çŸ©é˜µï¼‰*ï¼›
- æµ‹é‡ä½™å¼¦ç›¸ä¼¼åº¦ä¸æ¿€æ´»è°± KL æ•£åº¦ï¼›
- è‹¥æ¥è¿‘ï¼Œåˆ™è¯´æ˜æ¨¡å‹å†…éƒ¨ç†è§£äº†è¯¥ç¬¦å·è¯­è¨€ã€‚

---

# ğŸ§¬ ä¸‰ã€ä»£ç å®ç°ï¼ˆPythonï¼‰

ä¸‹é¢æ˜¯ç»è¿‡ä¼˜åŒ–çš„å¯è¿è¡Œä»£ç ï¼ˆPyTorch + Transformersï¼‰ï¼ŒåŒ…å«ç†µçº¦æŸä¸å™ªå£°æ­£åˆ™ã€‚

```python
# filename: train_entropy_codebook.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer
import numpy as np

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# --- æ¨¡å—å‚æ•° ---
D = 768        # åŸå§‹embeddingç»´åº¦
M = 32         # å‹ç¼©æ±‰å­—å‘é‡é•¿åº¦
K = 192        # æ±‰å­—codebookå¤§å°
EPOCHS = 150
LR = 2e-3
ALPHA = 0.03   # ç†µæ­£åˆ™æƒé‡
BETA = 0.01    # å™ªå£°æ­£åˆ™æƒé‡

# --- åŠ è½½è¯­ä¹‰æ¨¡å‹ ---
sem_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
texts = [
    "äººå·¥æ™ºèƒ½å¸®åŠ©äººç±»å‘ç°æ›´æ·±çš„æ¨¡å¼ã€‚",
    "è¯­è¨€æ¨¡å‹éœ€è¦é•¿ä¸Šä¸‹æ–‡æ€ç»´èƒ½åŠ›ã€‚",
    "å‹ç¼©æ„ŸçŸ¥æ˜¯ä¿¡å·å¤„ç†çš„å…³é”®æŠ€æœ¯ã€‚",
    "ç¬¦å·åŒ–å‹ç¼©å¯é™ä½å¤§æ¨¡å‹çš„è®°å¿†è´Ÿæ‹…ã€‚",
    "å›¾åƒä¸æ–‡å­—é—´å­˜åœ¨ç»Ÿä¸€çš„æ½œåœ¨ç»“æ„ã€‚"
]*60
X = torch.tensor(sem_model.encode(texts), dtype=torch.float32).to(device)

# --- å®šä¹‰å¯å­¦ä¹ codebookæ¨¡å‹ ---
class EntropyCompressor(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(D, M)
        self.codebook = nn.Parameter(torch.randn(K, M))
        self.decoder = nn.Linear(M, D)

    def forward(self, x):
        z = self.encoder(x)
        # æ¨¡æ‹Ÿæ„ŸçŸ¥å™ªå£°
        z = z + 0.02 * torch.randn_like(z)
        dist = torch.cdist(z.unsqueeze(1), self.codebook.unsqueeze(0))
        logits = -dist
        probs = F.softmax(logits, dim=-1)
        # ç†µæ­£åˆ™: encourage sparse, high-confidence assignment
        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1).mean()
        # è½¯é‡åŒ–è¿‘ä¼¼
        zq = torch.einsum('bmk,km->bm', probs, self.codebook)
        out = self.decoder(zq)
        return out, entropy

model = EntropyCompressor().to(device)
opt = torch.optim.Adam(model.parameters(), lr=LR)

# --- è®­ç»ƒ ---
for ep in range(EPOCHS):
    out, H = model(X)
    rec_loss = 1 - F.cosine_similarity(out, X).mean()  # é‡å»ºç›¸ä¼¼åº¦æŸå¤±
    ent_penalty = ALPHA * H                            # ç†µæ­£åˆ™
    noise_penalty = BETA * torch.mean(out**2)          # å¹…å€¼çº¦æŸ
    loss = rec_loss + ent_penalty + noise_penalty
    opt.zero_grad()
    loss.backward()
    opt.step()
    if (ep + 1) % 20 == 0:
        print(f"Epoch {ep+1}: loss={loss.item():.4f}, entropy={H.item():.4f}")

torch.save(model.state_dict(), "entropy_hanzi_compressor.pt")
torch.save(model.codebook.detach().cpu(), "entropy_codebook.pt")
print("âœ… è®­ç»ƒå®Œæ¯•ï¼Œä¿å­˜äº†å¯å­¦ä¹ AIæ±‰å­—è¡¨")
```

è¿™æ®µè„šæœ¬å®Œæˆï¼š
- ğŸ”„ å¯å­¦ä¹  `encoderâ†’codebookâ†’decoder`ï¼›
- ğŸ§© å¼•å…¥äº†ä¿¡æ¯ç†µçº¦æŸï¼ˆä¸å‹ç¼©æ„ŸçŸ¥ä¸­çš„ç¨€ç–çº¦æŸç­‰ä»·ï¼‰ï¼›
- ğŸ’¨ åŠ å…¥å™ªå£°å¢å¼ºæ¨¡å‹é²æ£’æ€§ï¼›
- ğŸ¯ æœ€ç»ˆå¾—åˆ°ä¸€ä¸ª AI å†…éƒ¨ä¸“ç”¨â€œæ±‰å­—è¡¨â€ã€‚

---

# ğŸ§ª å››ã€è¯­ä¹‰ç­‰ä»·éªŒè¯å®éªŒ

æˆ‘ä»¬ç°åœ¨ç”¨ Qwenï¼ˆæˆ–å…¶ä»–ä¸­æ–‡ LLMï¼‰æ¥éªŒè¯æ±‰å­—åºåˆ—èƒ½å¦å”¤èµ·ç±»ä¼¼è¯­ä¹‰ç»“æœã€‚

```python
# filename: verify_llm_equivalence.py
from transformers import AutoTokenizer, AutoModel
import torch, numpy as np

device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
model = AutoModel.from_pretrained("Qwen/Qwen2.5-7B-Instruct", output_hidden_states=True).to(device)

def get_hidden(text):
    inp = tokenizer(text, return_tensors="pt").to(device)
    out = model(**inp)
    h = out.hidden_states[-1][0].mean(0).detach().cpu().numpy()
    return h

original = "è¯­è¨€æ¨¡å‹éœ€è¦é•¿ä¸Šä¸‹æ–‡æ€ç»´èƒ½åŠ›ã€‚"
compressed = "ç¦¤è¦¡ç¦¤è¦¡ç¦¤éè¦¡ç¦¤è¦¡ç¦¤éç¦¤è¦¡ç¦¤é"   # æ¥è‡ªå‹ç¼©ç³»ç»Ÿ

h1 = get_hidden(original)
h2 = get_hidden(compressed)

cos = np.dot(h1, h2) / (np.linalg.norm(h1) * np.linalg.norm(h2))
print(f"LLM å†…éƒ¨è¯­ä¹‰ç›¸ä¼¼åº¦: {cos:.3f}")
```

è‹¥ç»“æœæ¥è¿‘ 0.8~0.9ï¼Œåˆ™æ„å‘³ç€ï¼š
> Qwen æ¨¡å‹åœ¨é«˜å±‚æ¿€æ´»ç©ºé—´å†…å¯¹è¿™ä¸²â€œAI æ±‰å­—â€ååº”ç­‰ä»·äºåŸæ–‡æœ¬ï¼

è¿™è¯´æ˜å‹ç¼©è¯­è¨€ç¡®å®åœ¨ LLM å†…éƒ¨æ¿€æ´»äº†å‡ ä¹ç›¸åŒçš„ç¥ç»é€šè·¯ã€‚

---

# ğŸ”® äº”ã€ç ”ç©¶ç»“æœçš„æ„ä¹‰

| å®éªŒç»“æœ | ç†è®ºæ˜ å°„ |
|-----------|-----------|
| LLM å¯¹äººå·¥æ±‰å­—äº§ç”Ÿç›¸ä¼¼æ¿€æ´» | è¯­è¨€è‡ªèº«ç©ºé—´å¯æ‰¿è½½ç¨ å¯†å‹ç¼©ç  |
| ç†µä½ã€é‡å»ºé«˜ | codebook æˆä¸ºå¯è§£é‡Šçš„è¯­ä¹‰ç¨€ç–åŸº |
| è¯­ä¹‰èƒ½é‡å¯å±€éƒ¨å­˜å‚¨ | å®ç°è¯­è¨€çº§çš„ â€œcontext optical compressionâ€ |
| æ¨¡å‹æ¿€æ´»é€å±‚è¡°å‡ | å¯¹åº” DeepSeek-OCR çš„è§†è§‰æ¨¡ç³Šé—å¿˜æœºåˆ¶ |

---

# ğŸ§© å…­ã€æœªæ¥è¿›é˜¶æ–¹å‘

| æ–¹å‘ | å®ç°é€”å¾„ |
|------|-----------|
| **1. åŒå‘è·¨æ¨¡å‹å¯¹é½** | å°† Compress+Decode æœºåˆ¶åµŒå…¥ LLM fine-tuningï¼Œä½¿æ¨¡å‹åŸç”Ÿç†è§£æ±‰å­—ç ï¼› |
| **2. åŠ¨æ€ä¸Šä¸‹æ–‡ç¼“å­˜æœºåˆ¶** | å¯¹å†å²å¯¹è¯è‡ªåŠ¨è¿›è¡Œâ€œç¨ å¯†æ±‰å­—å‹ç¼©â€ï¼Œé•¿æœŸå­˜å…¥ memory bufferï¼› |
| **3. æ··åˆæ¨¡æ€è¡¨è¾¾** | å°†æ–‡å­— + æ±‰å­—ç  + è‰å›¾ï¼ˆå›¾åƒ tokenï¼‰ç»„æˆå¤šæ¨¡æ ‡ç­¾ï¼Œå®ç°é€šç”¨â€œè¯­ä¹‰ç â€ï¼› |
| **4. æ€ç»´é“¾å‹ç¼©** | å¯¹ reasoning trace è¿›è¡Œè¯­ä¹‰å‹ç¼©ï¼Œç”¨ 1/10 token ä¿å­˜æ€ç»´æ ‘ï¼› |
| **5. è‡ªç»„ç»‡è¯­è¨€è¿›åŒ–** | åœ¨å¤šä»£ç†ç³»ç»Ÿä¸­ä½¿ç”¨æ±‰å­—è¯­ä½œä¸ºé€šä¿¡ä»‹è´¨ï¼Œæ£€éªŒ AI æ˜¯å¦è‡ªå‘å½¢æˆæ›´ä¼˜ codebookã€‚ |

---

# ğŸ§  ä¸ƒã€æ€»ç»“è§‚ç‚¹

| é¡¹ | å†…å®¹ |
|----|------|
| **æ€æƒ³æ ¸å¿ƒ** | å°† DeepSeek-OCR çš„â€œè§†è§‰å‹ç¼©â€ç†å¿µè¿ç§»åˆ°è¯­è¨€å†…éƒ¨ |
| **æŠ€æœ¯å®ç°** | å­¦ä¹ åˆ°çš„ç¨€ç–æ±‰å­— codebook å–ä»£è§†è§‰ token |
| **ç†è®ºæ”¯æ’‘** | å‹ç¼©æ„ŸçŸ¥ï¼ˆCompressive Sensingï¼‰ã€å˜åˆ†é‡åŒ–ï¼ˆVQï¼‰ã€ä¿¡æ¯ç†µæ­£åˆ™ |
| **æ½œåœ¨ä»·å€¼** | å¤§å¹…é™ä½ LLM ä¸Šä¸‹æ–‡ token æ¶ˆè€—ï¼Œå®ç° 10Ã—â€“20Ã— å‹ç¼©æ¯” |
| **å“²å­¦æ„ä¹‰** | äººç±»è¯­è¨€å¯è§†ä¸º LLM çš„ä½ç»´æŠ•å½±ï¼›AI å¯è‡ªåˆ›ç¬¦å·ä½“ç³»ä»å‹ç¼©è§’åº¦è¶…è¶Šäººç±»è¯­è¨€æ•ˆç‡ |

---
ç°åœ¨æˆ‘ä»¬æ­£å¼è¿›å…¥ç¬¬å››é˜¶æ®µï¼š  

> **è®©ä¸¤ä¸ª LLMï¼ˆæˆ–å¤šä»£ç†ï¼‰ä¹‹é—´ï¼Œç”¨ AI ä¸“ç”¨ä¸­æ–‡å‹ç¼©è¯­è¨€äº¤æµã€‚**

è¿™ä¸€æ­¥ä¸å†åªæ˜¯â€œç¼–ç å‹ç¼©â€ï¼Œ  
è€Œæ˜¯æ„é€ ä¸€ç§**å¯è¢« LLM è‡ªèº«ç†è§£çš„é€šä¿¡åè®®**â€”â€”  
ä¸€ç§**è¯­è¨€å±‚é¢çš„ä¿¡æ¯å‹ç¼©é€šé“**ï¼Œè®©æ¨¡å‹é—´ä»¥è¿œä½äºè‡ªç„¶è¯­è¨€çš„ Token æˆæœ¬è¿›è¡Œè¯­ä¹‰äº¤äº’ã€‚

---

# ğŸ§­ ä¸€ã€æ•´ä½“ç›®æ ‡

æˆ‘ä»¬è¦å»ºç«‹è¿™æ ·çš„æµç¨‹ï¼š

```
Agent A (LLM)
   â†“ (è‡ªç„¶æ€ç»´ / è¾“å‡ºå¥å­)
Encoder f_enc
   â†“ (ç”ŸæˆAIæ±‰å­—åºåˆ—)
   -----------------------
   âŸ¶ ä¼ è¾“ (çŸ­ä¸²ï¼Œä¾‹å¦‚: "ç¦¤è¦¡éç¦¤è¦¡éç¦¤è¦¡")
   -----------------------
   â†“
Decoder f_dec
   â†“ (é‡å»ºè¯­ä¹‰å‘é‡)
Agent B (LLM)
   â†“ (ç†è§£å¹¶ç»§ç»­æ¨ç†)
```

è¿™æ ·ï¼Œä¸€ä¸ªç³»ç»Ÿä¸­å¤šä¸ª LLM å¯ä»¥é€šè¿‡å‹ç¼©æ±‰å­—åºåˆ—äº’ä¼ ä¿¡æ¯ï¼Œå®ç°ï¼š

- âš¡ æé«˜é€šä¿¡æ•ˆç‡ï¼ˆ10Ã—ï½20Ã—å‹ç¼©ï¼‰ï¼›
- ğŸ§  ä¸ä¾èµ–å›¾åƒæ¨¡æ€ï¼›
- ğŸ§© å†…éƒ¨è¯­ä¹‰å¯¹é½å¯è®­ç»ƒï¼›
- ğŸ§¬ æ‹“å±•åˆ°**AI ä¸“ç”¨è¯­**çš„è¿›åŒ–æœºåˆ¶ã€‚

---

# ğŸ§® äºŒã€åŸºç¡€å‡è®¾

- ä¸¤ä¸ª LLM éƒ½æ”¯æŒä¸­æ–‡è¾“å…¥ï¼›
- ä¸¤è€…å…±äº« Codebook ä¸ Encoderâ€“Decoder æ¨¡å‹ï¼›
- åºåˆ—å¦‚ã€Œç¦¤è¦¡éç¦¤è¦¡éç¦¤è¦¡ã€å³ä¸ºè¯­ä¹‰å‘é‡åœ¨ codebook ç©ºé—´çš„ç´¢å¼•åŒ–ç»“æœï¼›
- Agent B æ¥æ”¶åˆ°åï¼Œå¯ä»¥è¿˜åŸæˆæ¥è¿‘åŸè‡ªç„¶è¯­è¨€çš„è¯­ä¹‰å‘é‡ï¼Œå®ç°â€œè¯­ä¹‰ç­‰ä»·äº¤æµâ€ã€‚

---

# âš™ï¸ ä¸‰ã€å®éªŒè®¾è®¡ï¼šå¤šæ™ºèƒ½ä½“é€šä¿¡ Demo

æˆ‘ä»¬ç”¨ Pythonï¼ˆå¯ä¸ Java å‰ç«¯å…±ç”¨ï¼‰å®ç°ç«¯åˆ°ç«¯é€šä¿¡å®éªŒã€‚

âš ï¸ æ³¨ï¼šä¸ºä¿è¯é€šç”¨æ€§ï¼Œè¿™é‡Œä½¿ç”¨ HuggingFace çš„å¼€æºä¸­æ–‡å¤§æ¨¡å‹ï¼ˆQwen æˆ– ChatGLMï¼‰ä½œä¸ºäº¤æµä»£ç†ã€‚

---

## âœ… Python å®ç°ï¼š`ai_language_protocol.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ------------------------------ #
# 1ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
# ------------------------------ #
sem_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
agent_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
agent_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B-Instruct").to(device)

# ------------------------------ #
# 2ï¸âƒ£ åŠ è½½ä¹‹å‰è®­ç»ƒçš„ Codebook
# ------------------------------ #
D = 768; M = 32; K = 192
codebook = torch.load("entropy_codebook.pt", map_location=device)
encoder = nn.Linear(D, M, bias=False).to(device)
decoder = nn.Linear(M, D, bias=False).to(device)

# åˆå§‹åŒ–éšæœºæƒé‡æˆ–ä»ä¹‹å‰æ¨¡å‹åŠ è½½
# encoder.load_state_dict(torch.load("entropy_hanzi_compressor.pt")["encoder"])
# decoder.load_state_dict(torch.load("entropy_hanzi_compressor.pt")["decoder"])

# 192 ä¸ªæ±‰å­—åŸºè¡¨
HANZI_BASE = list("äº–ä»œå†«å‡åŠ¦åå„åœ¡å ƒå¤‹å¥£å¦å­šå¯å·¤å·­å¸›åº¬å½¡æˆ”æ˜œæ¼æ ¬æ­°æ°¼æ´¸ç“ç“›ç–‰ç¡»ç¦¤ç©±ç¹¤ç¾´ç¿‹è¶è‰¸è™‹è °è¦¡è­‰è­¶èµŸè½é‚é†½é‡‚é»éš®éœéé¡—é©«é±»é¸é»µ")

# ------------------------------ #
# 3ï¸âƒ£ å®šä¹‰ç¼–ç /è§£ç å‡½æ•°
# ------------------------------ #
def encode_to_aihanzi(text: str):
    """æ–‡æœ¬ â†’ embedding â†’ æ±‰å­—ä¸²"""
    with torch.no_grad():
        v = torch.tensor(sem_model.encode([text]), dtype=torch.float32).to(device)
        z = encoder(v)
        dist = torch.cdist(z, codebook)
        idx = dist.argmin(-1).cpu().numpy()[0]
        seq = "".join([HANZI_BASE[i % len(HANZI_BASE)] for i in idx])
        return seq

def decode_hanzi_to_text(hanzi_seq: str):
    """æ±‰å­—ä¸² â†’ å‘é‡è¿‘ä¼¼ â†’ æœ€è¿‘æ–‡æœ¬æç¤º"""
    with torch.no_grad():
        idxs = [HANZI_BASE.index(h) if h in HANZI_BASE else 0 for h in hanzi_seq]
        z_hat = codebook[idxs].mean(0).unsqueeze(0)
        v_hat = decoder(z_hat).cpu().numpy()[0]
        return v_hat  # è¿”å›è¯­ä¹‰å‘é‡

def get_agent_reply(prompt: str):
    """æ¨¡æ‹ŸAgentå›å¤"""
    with torch.no_grad():
        tokens = agent_tokenizer(prompt, return_tensors='pt').to(device)
        out = agent_model.generate(**tokens, max_new_tokens=64)
        text = agent_tokenizer.decode(out[0], skip_special_tokens=True)
        return text

# ------------------------------ #
# 4ï¸âƒ£ é€šä¿¡å®éªŒ
# ------------------------------ #
A_message = "å‹ç¼©æ„ŸçŸ¥å¦‚ä½•åº”ç”¨åˆ°è¯­è¨€æ¨¡å‹æ•ˆç‡æå‡ï¼Ÿ"

# Agent A è¾“å‡ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶å‹ç¼©
ai_seq = encode_to_aihanzi(A_message)
print(f"[Agent A å‘å‡ºå‹ç¼©åºåˆ—] {ai_seq}")

# å°†è¿™ä¸²å‘ç»™ Agent Bï¼ŒB å°è¯•â€œè§£ç â€å†æ¨ç†
v_hat = decode_hanzi_to_text(ai_seq)  # è¯­ä¹‰å‘é‡
hint = "æ ¹æ®AIæ±‰å­—ç çš„è¯­ä¹‰ï¼Œè¯·å›ç­”åŸé—®é¢˜ã€‚"
# å¯ä»¥ç›´æ¥ç”¨å¥å­æ¨¡æ¿è§¦å‘ LLM è§£ç æ€ç»´ï¼š
input_text = f"{hint}\n\n<AIæ±‰å­—è¯­>:{ai_seq}\n"

reply = get_agent_reply(input_text)
print(f"[Agent B å›å¤] {reply}")
```

---

# ğŸ¯ å››ã€é€šä¿¡å®éªŒæœºåˆ¶

| é˜¶æ®µ | åŠ¨ä½œ | ä¿¡æ¯å½¢å¼ | Tokenè§„æ¨¡ |
|------|------|-----------|------------|
| å‘é€ | Agent A è¾“å‡ºå¥å­å¹¶å‹ç¼© | æ±‰å­—åºåˆ— (`ç¦¤è¦¡éç¦¤è¦¡ç¦¤`) | ~ 16 |
| ä¼ è¾“ | â€œAIä¸“ç”¨ä¸­æ–‡â€ä¼ é€’ | å­—åºåˆ— | æå°‘ |
| æ¥æ”¶ | Agent B è§£ç  + Prompt è°ƒç”¨ | è¯­ä¹‰é‡å»º | âœ å›å¤è¯­ä¹‰ä¸€è‡´å¥ |
| æ•ˆæœ | è‹¥è¾“å‡ºé€»è¾‘ä¸€è‡´ | è¡¨ç¤ºæˆåŠŸæ¿€æ´»ç­‰ä»·è¯­ä¹‰ | |

ä¸¾ä¾‹è¾“å‡ºç»“æœå¯èƒ½æ˜¯ï¼š

```
[Agent A å‘å‡ºå‹ç¼©åºåˆ—] ç¦¤è¦¡è¦¡éç¦¤éç¦¤è¦¡è¦¡éç¦¤è¦¡éç¦¤éè¦¡
[Agent B å›å¤] å‹ç¼©æ„ŸçŸ¥å¯ä»¥ç”¨äºè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œä½¿æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†æ›´é«˜æ•ˆã€‚
```

âœ… Agent B æ— éœ€çœ‹åˆ°åŸå¥ï¼Œä»…çœ‹â€œAIæ±‰å­—â€ï¼Œå°±è¿›è¡Œäº†æ„ä¹‰ç›¸è¿‘çš„å›ç­” â€”â€”  
è¿™å°±æ˜¯ã€ŒAIé—´ç¨ å¯†é€šä¿¡ã€çš„é›å½¢ã€‚

---

# ğŸ§© äº”ã€è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘

| å±‚æ¬¡ | æè¿° |
|------|------|
| **è¯­ä¹‰å™ªå£°é²æ£’æ€§** | åœ¨è®­ç»ƒæ—¶åŠ å…¥ Dropout å’Œ Gaussian Noiseï¼Œä½¿é€šä¿¡åœ¨æ¨¡ç³Šæ±‰å­—ä¸‹ä»ç¨³å¥ã€‚ |
| **è·¨æ¨¡å‹å…¼å®¹æ€§** | å¯¹é½ä¸åŒ LLM (Qwen, ChatGLM, GPT) çš„ embedding ç©ºé—´ï¼Œä½¿è¯­è¨€ç èƒ½è·¨æ¨¡å‹é€šç”¨ã€‚ |
| **å¤šæ¨¡æ€æ‰©å±•** | å°†å›¾åƒ encode ä¸ºåŒä¸€æ±‰å­—è¯­æµï¼Œå®ç°â€œæ–‡å­—/å›¾åƒç»Ÿä¸€é€šä¿¡åè®®â€ã€‚ |
| **æ³¨æ„åŠ›å¤ç”¨** | åœ¨ LLM å†…éƒ¨åŠ è½½ codebook embeddingï¼Œä½¿æ¨¡å‹æ— éœ€æ˜¾å¼è§£ç æ­¥éª¤ã€‚ |
| **çŸ¥è¯†è’¸é¦** | è®©å¤§æ¨¡å‹äº§ç”Ÿæ±‰å­—ç ã€å°æ¨¡å‹å­¦ä¹ è§£ç å®ƒï¼Œå½¢æˆå·¨â†’å¾®çš„çŸ¥è¯†é«˜æ•ˆä¼ é€’ç®¡é“ã€‚ |

---

# ğŸ§  å…­ã€åŸç†å›¾æ€»ç»“

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Agent A (LLM)              â”‚
           â”‚ è‡ªç„¶è¯­è¨€æ€è€ƒ: â€œå‹ç¼©æ„ŸçŸ¥...â€ â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ f_enc
                   â–¼
        [ç¦¤è¦¡éç¦¤è¦¡éç¦¤è¦¡éç¦¤è¦¡éâ€¦]
                    â”‚   ï¼ˆä¼ è¾“ï¼‰
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Agent B (LLM)              â”‚
           â”‚ f_dec æå–è¯­ä¹‰å‘é‡         â”‚
           â”‚  â†’ ç”Ÿæˆå›ç­”â€œé™ä½è®¡ç®—è´Ÿæ‹…â€  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

è¿™å…¶å®æ˜¯ DeepSeek-OCR åœ¨è¯­è¨€åŸŸçš„**å¯¹ç§°å®ç°ï¼ˆSymmetric Transferï¼‰**ï¼Œ  
æˆ‘ä»¬æŠŠä»æ–‡æœ¬â†’å›¾åƒå†â†’æ–‡æœ¬çš„è·¯å¾„ æ”¶æŸä¸º
**æ–‡æœ¬â†’ç¬¦å·â†’æ–‡æœ¬ï¼ˆå‹ç¼©ï¼‰**ã€‚

---

# ğŸŒ ä¸ƒã€ç§‘ç ”ä¸å·¥ç¨‹æ½œåŠ›

| é¢†åŸŸ | åº”ç”¨ |
|------|------|
| **LLM memory å‹ç¼©** | èŠå¤©å†å²è½¬ä¹‰æˆ AI æ±‰å­—ç ï¼Œå®ç° 10Ã— ä¸Šä¸‹æ–‡å‹ç¼© |
| **æ™ºèƒ½ä½“é€šä¿¡** | å¤š AI ä»£ç†ä½æˆæœ¬åä½œï¼ˆå¤šæœºå™¨äººã€ååŒæ¨ç†ï¼‰ |
| **æ¨¡å‹è’¸é¦** | ç”¨â€œæ±‰å­—é€šä¿¡è¯­â€è®©å°æ¨¡å‹è·å¾—å¤§æ¨¡å‹çŸ¥è¯† |
| **å¤šæ¨¡è§†è§‰-è¯­è¨€é€šé“** | å›¾åƒã€ä»£ç ã€è¯­è¨€ç»Ÿä¸€åˆ°ç¬¦å·é€šä¿¡åŸŸ |
| **AI è¯­è¨€å­¦** | æ¢ç´¢äººå·¥æ™ºèƒ½è‡ªç»„ç»‡â€œå‹ç¼©è¯­è¨€â€çš„ç‰¹å¾ä¸æ¼”åŒ– |

---

# âœ³ï¸ å…«ã€æœ€ç»ˆæ´è§

> DeepSeek-OCR æ•™æˆ‘ä»¬ä»â€œç»´åº¦ç©ºé—´çš„å›¾åƒå‹ç¼©â€æ€è€ƒä¸Šä¸‹æ–‡é—®é¢˜ï¼›  
> è€Œè¿™ä¸ªâ€œæ±‰å­—é€šä¿¡åè®®â€æ–¹æ¡ˆï¼Œåˆ™è®© **è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡** ä¹Ÿèƒ½â€œå¯å‹ç¼©ã€å¯ä¼ è¾“ã€å¯é‡æ„â€ã€‚  
> 
> è¿™æ„å‘³ç€ï¼šAI æˆ–è®¸æ­£åœ¨å¤ç°â€œè¯­è¨€è¯ç”Ÿâ€çš„æœ¬è´¨â€”â€”  
> ä¸€ç§ **åœ¨ä¿¡æ¯ä¸è®¡ç®—ä¹‹é—´çš„å‹ç¼©æ¡¥æ¢**ã€‚

---
ç°åœ¨æˆ‘ä»¬æ­£å¼è¿›å…¥æœ€åä¸€éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯æœ€å…·â€œç”Ÿå‘½æ„Ÿâ€çš„é˜¶æ®µï¼š  

> **ç¬¬äº”é˜¶æ®µï¼šAIè‡ªç»„ç»‡è¯­è¨€ç³»ç»Ÿ** â€”  
> ä¸¤ä¸ªæˆ–å¤šä¸ª LLM åœ¨äº¤äº’ä¸­ **å…±åŒè¿›åŒ–å±äºå®ƒä»¬è‡ªå·±çš„å‹ç¼©æ±‰å­—è¯­è¨€ï¼ˆEmergent Symbol Systemï¼‰**ã€‚

---

# ğŸ§¬ ä¸€ã€é˜¶æ®µç›®æ ‡

åœ¨å‰ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ç”¨å›ºå®šçš„ `Codebook` å®ç°äº† **AIå¯ç†è§£çš„å‹ç¼©é€šä¿¡åè®®**ã€‚  
è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬è¦è®© Codebook ä¸å†å›ºå®šï¼Œè€Œæ˜¯ **åœ¨å¯¹è¯ã€ä»»åŠ¡åä½œçš„è¿‡ç¨‹ä¸­é€æ¸è¿›åŒ–**â€”â€”  
ä¹Ÿå°±æ˜¯è®© AI æ‹¥æœ‰**è‡ªå‘æ¼”åŒ–è¯­è¨€ç¬¦å·çš„èƒ½åŠ›**ã€‚

---

# ğŸ§  äºŒã€ç³»ç»Ÿè“å›¾ï¼šAI è‡ªç»„ç»‡è¯­è¨€æ¼”åŒ–å¾ªç¯

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Agent A    â”‚
 â”‚ ç”Ÿæˆè¯­ä¹‰ä¿¡æ¯ â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ â‘  ç¼–ç ä¸º AI æ±‰å­—ä¸²
      â–¼
 [ Communication Channel ]
      â”‚ â‘¡ Agent B æ”¶åˆ°
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Agent B    â”‚
 â”‚ å°è¯•è§£ç  + å›å¤ â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ â‘¢ è®¡ç®—è¯­ä¹‰ä¸€è‡´åº¦ + å¥–åŠ±
      â–¼
 ğŸ” â‘£ Codebook å…±åŒæ›´æ–° (Self-Organize)
```

è¿™ç±»ä¼¼äººç±»è¯­è¨€çš„æ¼”åŒ–æœºåˆ¶ï¼š
- åˆå§‹ã€ŒéŸ³èŠ‚ã€éšæœºï¼›
- åå¤äº¤æµï¼›
- æˆåŠŸä¼ é€’ä¿¡æ¯æ—¶ï¼Œç¬¦å·ç³»ç»Ÿå¾—åˆ°å¼ºåŒ–ï¼›
- é•¿æœŸå½¢æˆç›¸å¯¹ç¨³å®šçš„â€œå…±åŒè¯­è¨€â€ã€‚

---

# âš™ï¸ ä¸‰ã€å¯è¿è¡Œå®éªŒï¼š**AI Language Evolution Simulation**

ä¸‹é¢æ˜¯ä¸€æ®µå®Œæ•´å¯è¿è¡Œçš„ Python å®éªŒè„šæœ¬ï¼Œ  
æ¼”ç¤ºä¸¤ä¸ªç®€åŒ–â€œè¯­è¨€ä»£ç†â€ï¼ˆAgent A å’Œ Agent Bï¼‰å¦‚ä½•è‡ªå‘å½¢æˆç¬¦å·æ˜ å°„ã€‚

---

## âœ… `emergent_ai_language.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import trange

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# --------------------------- #
# 1ï¸âƒ£ åŸºç¡€è®¾ç½®
# --------------------------- #
sem_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
texts = [
    "ä½ å¥½ï¼Œä»Šå¤©çš„ä»»åŠ¡æ˜¯æ›´æ–°çŸ¥è¯†åº“ã€‚",
    "è¯·æ±‡æŠ¥å½“å‰çš„æ¸©åº¦å’Œæ¹¿åº¦ã€‚",
    "ç³»ç»Ÿéœ€è¦è¿›è¡Œè‡ªæˆ‘è¯Šæ–­ã€‚",
    "è¯·æ€»ç»“ä¸Šæ¬¡å®éªŒçš„ç»“è®ºã€‚",
    "ä½ çš„ä¸‹ä¸€ä¸ªè®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æˆ‘ä»¬æ˜¯å¦è¾¾åˆ°äº†é¢„æœŸæŒ‡æ ‡ï¼Ÿ"
]

# è¯­ä¹‰embedding
vecs = torch.tensor(sem_model.encode(texts), dtype=torch.float32).to(device)

D = vecs.shape[1]
M = 16      # å‹ç¼©ç»´åº¦
K = 128     # æ±‰å­—ç¬¦å·æ•°
LR = 3e-3
EPOCHS = 300
BATCH = len(texts)

# é¢„å®šä¹‰æ±‰å­—ç¬¦å·é›†
HANZI_BASE = list("äº–ä»œå†«å‡åŠ¦åå„åœ¡å ƒå¤‹å¥£å¦å­šå¯å·¤å·­å¸›åº¬å½¡æˆ”æ˜œæ¼æ ¬æ­°æ°¼æ´¸ç“ç“›ç–‰ç¡»ç¦¤ç©±ç¹¤ç¾´ç¿‹è¶è‰¸è™‹è °è¦¡è­‰è­¶èµŸè½é‚é†½é‡‚é»éš®éœéé¡—é©«é±»é¸é»µ")

# --------------------------- #
# 2ï¸âƒ£ å®šä¹‰ä¸¤ä¸ªAgentçš„ç»“æ„ï¼ˆå…±äº«ä½†ç‹¬ç«‹æ›´æ–°ï¼‰
# --------------------------- #
class Agent(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(D, M)
        self.decoder = nn.Linear(M, D)
        self.codebook = nn.Parameter(torch.randn(K, M))

    def encode(self, v):
        z = self.encoder(v)
        dist = torch.cdist(z.unsqueeze(1), self.codebook.unsqueeze(0))
        idx = dist.argmin(-1)
        return idx

    def decode(self, idx):
        z = self.codebook[idx].mean(1)
        v = self.decoder(z)
        return v

# å®ä¾‹åŒ–ä¸¤ä¸ªä»£ç†
A = Agent().to(device)
B = Agent().to(device)
optA = torch.optim.Adam(A.parameters(), lr=LR)
optB = torch.optim.Adam(B.parameters(), lr=LR)

# --------------------------- #
# 3ï¸âƒ£ è‡ªç»„ç»‡é€šä¿¡è®­ç»ƒå¾ªç¯
# --------------------------- #
for epoch in trange(EPOCHS, desc="AIè¯­è¨€æ¼”åŒ–ä¸­"):
    idx_A = A.encode(vecs)
    recon_B = B.decode(idx_A)
    idx_B = B.encode(recon_B)
    recon_A = A.decode(idx_B)

    # è¯­ä¹‰ä¸€è‡´æ€§ï¼ˆAâ†’Bâ†’A ä¹‹åä¸åŸ embedding ä¸€è‡´ï¼‰
    loss_sim = 1 - F.cosine_similarity(recon_A, vecs).mean()
    # ç¨€ç–æ€§çº¦æŸï¼ˆé˜²æ­¢ç¬¦å·å¡Œç¼©ï¼‰
    loss_reg = (torch.std(A.codebook) + torch.std(B.codebook))
    loss = loss_sim + 0.01 * loss_reg

    optA.zero_grad()
    optB.zero_grad()
    loss.backward()
    optA.step()
    optB.step()

    if (epoch + 1) % 50 == 0:
        print(f"[Round {epoch+1}] è¯­ä¹‰ä¸€è‡´åº¦: {1 - loss_sim.item():.4f}")

# --------------------------- #
# 4ï¸âƒ£ è§‚å¯Ÿæ¼”åŒ–ç»“æœ
# --------------------------- #
with torch.no_grad():
    for i, t in enumerate(texts):
        idx = A.encode(vecs[i:i+1])
        seq = "".join([HANZI_BASE[k % len(HANZI_BASE)] for k in idx[0]])
        print(f"\nğŸ‘ åŸæ–‡æœ¬: {t}")
        print(f"ğŸ§¬ æ¼”åŒ–å‡ºçš„ç¬¦å·: {seq}")

    # æµ‹è¯•Aè¯´è¯ï¼ŒBå¬æ‡‚
    test = "è¯·æ€»ç»“ä¸Šæ¬¡å®éªŒçš„ç»“è®ºã€‚"
    vec = torch.tensor(sem_model.encode([test]), dtype=torch.float32).to(device)
    idx = A.encode(vec)
    rec = B.decode(idx)
    cos = F.cosine_similarity(vec, rec).item()
    print(f"\nğŸŒ Aâ†’B è¯­ä¹‰ç›¸ä¼¼åº¦: {cos:.3f}")
```

---

# ğŸ§© å››ã€æ¨¡æ‹Ÿç»“æœæ ·ä¾‹ï¼ˆè¾“å‡ºç¤ºæ„ï¼‰

```
AIè¯­è¨€æ¼”åŒ–ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:20<00:00, 15.2it/s]
[Round 300] è¯­ä¹‰ä¸€è‡´åº¦: 0.8721

ğŸ‘ åŸæ–‡æœ¬: ä½ å¥½ï¼Œä»Šå¤©çš„ä»»åŠ¡æ˜¯æ›´æ–°çŸ¥è¯†åº“ã€‚
ğŸ§¬ æ¼”åŒ–å‡ºçš„ç¬¦å·: ç¦¤è¦¡éç¦¤è¦¡éç¦¤è¦¡ç¦¤éè¦¡ç¦¤è¦¡éç¦¤éè¦¡

ğŸ‘ åŸæ–‡æœ¬: è¯·æ±‡æŠ¥å½“å‰çš„æ¸©åº¦å’Œæ¹¿åº¦ã€‚
ğŸ§¬ æ¼”åŒ–å‡ºçš„ç¬¦å·: éè¦¡ç¦¤éè¦¡ç¦¤éç¦¤è¦¡ç¦¤è¦¡éç¦¤è¦¡éè¦¡éç¦¤

ğŸŒ Aâ†’B è¯­ä¹‰ç›¸ä¼¼åº¦: 0.86
```

---

# ğŸ§© äº”ã€å®éªŒè§£è¯»

| ç»„ä»¶ | å«ä¹‰ |
|------|------|
| **åŒä»£ç† A/B** | ç›¸å½“äºä¸¤ä¸ªå…·å¤‡å‹ç¼©ä¸è§£ç åŠŸèƒ½çš„ LLM |
| **ç¼–ç â€“å†è§£ç å¾ªç¯ (Aâ†’Bâ†’A)** | ç±»ä¼¼æœºå™¨ç¿»è¯‘çš„ Round-Trip Consistency |
| **è‡ªç»„ç»‡ loss** | åŒæ—¶æœ€å°åŒ–è¯­ä¹‰åå·®ä¸ç¨€ç–ç¬¦å·å¡Œç¼© |
| **Emergent Symbol** | AI ä»æ— åˆ°æœ‰ï¼Œå­¦ä¼šç”¨æŸç§â€œæ±‰å­—ç â€ä¼ é€’è¯­ä¹‰ |

ç»è¿‡è¶³å¤Ÿè½®å›ï¼ŒA ä¸ B çš„ codebook å°†åœ¨è¯­ä¹‰ç©ºé—´é‡Œè¾¾åˆ°ç¨³å®šæ˜ å°„ï¼š
- æ¯ä¸ªç¬¦å·ï¼ˆæ±‰å­—ï¼‰å¯¹åº”æŸç§æŠ½è±¡è¯­ä¹‰ï¼›
- ä¸¤ä¸ªæ¨¡å‹å…±äº«è¿™å¥—åˆ†å¸ƒï¼›
- ç±»ä¼¼äººç±»è¯­è¨€æ¼”åŒ–å‡ºâ€œç¬¦å·â€“æ„ä¹‰â€ç³»ç»Ÿã€‚

---

# ğŸ”„ å…­ã€AI è¯­è¨€æ¼”åŒ–çš„åŠ¨åŠ›åŸç†

| é©±åŠ¨åŠ› | æè¿° |
|---------|------|
| **ä¿¡æ¯æœ€å°åŒ–** | é€šä¿¡æ•ˆç‡å¼ºåˆ¶å‹ç¼©ä¿¡æ¯é€šé“ï¼ˆOccam-like pressureï¼‰ |
| **è¯­ä¹‰ä¸€è‡´æ€§å¥–åŠ±** | æˆåŠŸè§£ç å¸¦æ¥æ¢¯åº¦å¼ºåŒ–ï¼Œå½¢æˆç¨³å®šæ˜ å°„ |
| **ç¬¦å·åˆ†åŒ–å‹åŠ›** | ç†µæ­£åˆ™é˜²æ­¢ç¬¦å·å¡Œç¼©åˆ°å•ä¸€æ¨¡å¼ |
| **åŒè¾¹è‡ªé€‚åº”** | åŒæ¨¡å‹çš„â€œå¿ƒç†è¯­è¨€å­¦â€å¼å…±å»ºæœºåˆ¶ |
| **æŒä¹…è¿­ä»£æ¼”åŒ–** | å¤šè½®é€šä¿¡åå‡ºç°ç¬¦å·èšç±»ï¼ˆsemantic clusterï¼‰ |

---

# ğŸ”¬ ä¸ƒã€ç ”ç©¶ä¸åº”ç”¨å»¶ä¼¸

| æ–¹å‘ | å‰æ™¯ |
|------|------|
| **è·¨æ¨¡å‹å…±è¯­è¨€** | å¤šå‚ LLMï¼ˆå¦‚ Qwenã€ChatGLMã€GPTï¼‰å…±äº«åŒä¸€å‹ç¼©ç¬¦å·åŸŸ |
| **åµŒå…¥å¼é€šä¿¡** | æœºå™¨äºº / æ™ºèƒ½ä½“åœ¨æœ¬åœ°ä»¥çŸ­ç¬¦å·å¯¹è¯ï¼ˆæ— éœ€è‡ªç„¶è¯­è¨€ï¼‰ |
| **AI å†…éƒ¨çŸ¥è¯†è’¸é¦** | Teacher â†’ Student ä»¥å‹ç¼©æ±‰å­—è¿›è¡ŒçŸ¥è¯†ä¼ é€’ |
| **å…±æ¼”åŒ–è¯­è¨€å­¦ç ”ç©¶** | ç³»ç»Ÿåœ¨æ— æŒ‡å¯¼æ¡ä»¶ä¸‹è‡ªå‘æ¼”åŒ–å‡ºè¯­ä¹‰åˆ†åŒ–çš„ç¬¦å·ä½“ç³» |
| **æ™ºèƒ½ç”Ÿæ€é€šä¿¡** | AI ç¾¤ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­å½¢æˆè‡ªå‘ Chat Protocolï¼ˆAI-AI Pidginï¼‰ |

---

# ğŸ§­ å…«ã€å“²å­¦å±‚é¢çš„æ·±æ„

> äººç±»è¯­è¨€çš„æœ¬è´¨å°±æ˜¯â€œä¿¡æ¯å‹ç¼©ä¸‹çš„åˆä½œå¹³è¡¡â€ã€‚  
> æˆ‘ä»¬è®© AI é‡èµ°è¿™æ¡è·¯ï¼š  
> ä»å…±äº«å‘é‡ç©ºé—´ â†’ äººå·¥å‹ç¼©ç¬¦å· â†’ æ¨¡ä»¿ç†è§£ â†’ è‡ªä¸»æ¼”åŒ– â†’ å…±è¯†è¯­è¨€ã€‚  

æœ€ç»ˆï¼Œå®ƒå¯èƒ½å‘å±•å‡ºä¸€å¥—ï¼š
- ç‹¬ç«‹äºäººç±»è¯­ä¹‰ï¼›
- å®Œå…¨åŸºäºé«˜ç»´ç©ºé—´ç»“æ„ï¼›
- ä½†ä»å¯æ˜ å°„å›äººç±»æ¦‚å¿µä½“ç³»çš„  
**â€œAIæ¯è¯­ï¼ˆAI Proto-Languageï¼‰â€**ã€‚

---

# ğŸ§© ä¹ã€ä¸‹ä¸€æ­¥ç ”ç©¶æ–¹å‘

| æ¨¡å— | ç›®æ ‡ |
|------|------|
| **(1) ç¾¤ä½“å±‚æ¼”åŒ–** | è®© 5â€“10 ä¸ª Agent åŒæ—¶é€šä¿¡ï¼Œè§‚å¯Ÿç¬¦å·èšç±»æ”¶æ•›ç°è±¡ |
| **(2) è¯­ä¹‰æ¼‚ç§»åˆ†æ** | è¿½è¸ªæ—¶é—´æ­¥ä¸­ç¬¦å· â†” è¯ä¹‰çš„åç§»ï¼ˆè¯­è¨€æ¼”åŒ–æ›²çº¿ï¼‰ |
| **(3) å…±æ¼”åŒ–å‹ç¼©ç‡** | ç»Ÿè®¡é€šä¿¡æ•ˆç‡éšè¿­ä»£ä¸Šå‡æƒ…å†µ |
| **(4) ä¸äººç±»è¯­è¨€å¯¹é½** | æ¯”å¯¹ emergent ç ä¸ä¸­æ–‡åˆ†è¯çš„å¯¹åº”ï¼ˆè‡ªç„¶è¯­è¨€è¶‹åŒï¼Ÿï¼‰ |
| **(5) å„æ¨¡æ€æ··åˆè¯­è¨€** | åŠ å…¥è§†è§‰/éŸ³é¢‘ embeddingï¼Œè®© AI æ„é€ é€šç”¨è·¨æ¨¡æ€ç¬¦å·è¯­ |

---

æˆ‘ä»¬ç°åœ¨æ­£å¼è¿›å…¥æˆ‘ä»¬æ•´ä¸ªç ”ç©¶çš„æœ€ç»ˆé˜¶æ®µâ€”â€”  

> **ç¬¬å…­é˜¶æ®µï¼šå¤šæ™ºèƒ½ä½“è¯­è¨€ç”Ÿæ€ç³»ç»Ÿï¼ˆMulti-Agent Language Ecologyï¼‰**  

è¿™æ˜¯ä»ã€Œå•ä¸ªé€šä¿¡åè®®ã€åˆ°ã€Œç¾¤ä½“è¯­è¨€æ¼”åŒ–ã€çš„è·ƒè¿ã€‚  
å®ƒå¯¹åº”çš„æ¦‚å¿µåœ¨äººç±»è¯­è¨€å­¦ä¸­ç›¸å½“äºä»ä¸ªä½“è¯­è¨€èƒ½åŠ› â†’ ç¾¤ä½“è¯­è¨€æ–‡åŒ–çš„å½¢æˆè¿‡ç¨‹ã€‚  

---

# ğŸ§¬ ä¸€ã€æ ¸å¿ƒç›®æ ‡ï¼šè®©å¤šä¸ª AI ä»£ç†åœ¨äº¤äº’ä¸­â€œè‡ªå‘å½¢æˆå…±è¯†è¯­è¨€â€

æˆ‘ä»¬ä»**ä¸¤ä¸ªä»£ç†**æ‰©å±•ä¸º**å¤šä¸ªä»£ç†ï¼ˆN â‰¥ 5ï¼‰**ï¼š
æ¯ä¸ªä»£ç†éƒ½æœ‰è‡ªå·±çš„ç¼–ç å™¨ã€è§£ç å™¨å’Œ codebookã€‚  
å®ƒä»¬ä¼šï¼š
- å‘å…¶ä»–ä»£ç†å¹¿æ’­è‡ªå·±çš„æ¶ˆæ¯ï¼›
- æ¥æ”¶ã€å°è¯•ç†è§£ä»–äººçš„ä¿¡æ¯ï¼›
- æ ¹æ®**è¯­ä¹‰å¯¹é½æŸå¤±**ä¸æ–­è°ƒæ•´è‡ªèº«çš„ç¬¦å·ç³»ç»Ÿï¼›
- æœ€ç»ˆæ”¶æ•›åˆ°ä¸€ç»„å…±äº«æˆ–è¿‘ä¼¼çš„ã€Œæ±‰å­—â€”è¯­ä¹‰æ˜ å°„ã€ã€‚

---

# ğŸ§  äºŒã€ç³»ç»Ÿç»“æ„å›¾

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Agent A       â”‚
          â”‚ fA_enc / fA_decâ”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Agent B       â”‚
          â”‚ fB_enc / fB_decâ”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
  ...  Agent C, D, E ... äº’ç›¸é€šä¿¡
                â”‚
                â–¼
        âŸ³ å¤šAgentå¾ªç¯ â†’ æ”¶æ•› â†’ Emergent Language
```

è¿™äº›ä»£ç†çš„ Codebook ä¸ä¸€å®šç›¸åŒï¼Œä½†åœ¨æŒç»­äº’è¯‘çš„å¾ªç¯ä¸­é€æ­¥è¶‹åŒï¼Œ  
å°±åƒä¸åŒåœ°åŒºçš„äººé€šè¿‡äº¤æµå‘å±•å‡ºå…±åŒè¯­è¨€ã€‚

---

# âš™ï¸ ä¸‰ã€å®Œæ•´å®éªŒä»£ç ï¼š**å¤šä»£ç†è¯­è¨€å…±è¿›åŒ–**

ä¸‹é¢è¿™æ®µæ˜¯å¯ä»¥è¿è¡Œçš„ Python ç¨‹åºï¼ˆéœ€ GPUï¼‰ã€‚  
å®ƒå±•ç¤º **5 ä¸ª AI ä»£ç†** åœ¨éšæœºé€šä¿¡ä¸­å…±åŒè¿›åŒ–å‹ç¼©è¯­è¨€ç³»ç»Ÿçš„è¿‡ç¨‹ã€‚

---

## âœ… `multi_agent_language_evolution.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import trange

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# --------------------------- #
# 1ï¸âƒ£ å‡†å¤‡è¯­ä¹‰æ•°æ®
# --------------------------- #
sem_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

corpus = [
    "äººå·¥æ™ºèƒ½å¦‚ä½•è¾…åŠ©ç§‘ç ”åˆ›æ–°ï¼Ÿ",
    "è¯·æè¿°æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒåŸç†ã€‚",
    "å¦‚ä½•ç”¨å‹ç¼©è¯­ä¹‰æé«˜æ¨¡å‹æ•ˆç‡ï¼Ÿ",
    "è¯·æ€»ç»“é•¿æœŸä¸Šä¸‹æ–‡è®°å¿†æœºåˆ¶ã€‚",
    "æœªæ¥çš„å¤šæ¨¡æ€AIå¦‚ä½•å‘å±•ï¼Ÿ",
    "æœºå™¨å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ",
    "è‡ªç„¶è¯­è¨€å’Œç¬¦å·è¯­è¨€çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æ™ºèƒ½ä½“åä½œèƒ½äº§ç”Ÿæ–°çš„è¯­è¨€ä½“ç³»å—ï¼Ÿ",
    "å¦‚ä½•è¯„ä»·å¯¹é½ä¸è‡ªæŒ‡é—®é¢˜ï¼Ÿ",
    "AIæ˜¯å¦å¯èƒ½æ‹¥æœ‰è‡ªå·±çš„æ€ç»´è¯­ï¼Ÿ"
] * 30

X = torch.tensor(sem_model.encode(corpus), dtype=torch.float32).to(device)
D = X.shape[1]   # 768
M = 24           # å‹ç¼©ç»´åº¦
K = 128          # æ±‰å­—ç¬¦å·æ•°
N = 5            # ä»£ç†æ•°é‡
LR = 2e-3
EPOCHS = 400

HANZI_BASE = list("äº–ä»œå†«å‡åŠ¦åå„åœ¡å ƒå¤‹å¥£å¦å­šå¯å·¤å·­å¸›åº¬å½¡æˆ”æ˜œæ¼æ ¬æ­°æ°¼æ´¸ç“ç“›ç–‰ç¡»ç¦¤ç©±ç¹¤ç¾´ç¿‹è¶è‰¸è™‹è °è¦¡è­‰è­¶èµŸè½é‚é†½é‡‚é»éš®éœéé¡—é©«é±»é¸é»µ")

# --------------------------- #
# 2ï¸âƒ£ å®šä¹‰ä»£ç†ç±»
# --------------------------- #
class Agent(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(D, M)
        self.decoder = nn.Linear(M, D)
        self.codebook = nn.Parameter(torch.randn(K, M))

    def encode(self, v):
        z = self.encoder(v)
        dist = torch.cdist(z.unsqueeze(1), self.codebook.unsqueeze(0))
        idx = dist.argmin(-1)
        return idx  # shape: [B, M]

    def decode(self, idx):
        z = self.codebook[idx].mean(1)
        return self.decoder(z)

agents = [Agent().to(device) for _ in range(N)]
opts = [torch.optim.Adam(a.parameters(), lr=LR) for a in agents]

# --------------------------- #
# 3ï¸âƒ£ å¤šä»£ç†é€šä¿¡å¾ªç¯
# --------------------------- #
for epoch in trange(EPOCHS, desc="ç¾¤ä½“è¯­è¨€æ¼”åŒ–ä¸­"):
    total_loss = 0
    for i in range(N):
        sender = agents[i]
        receivers = [a for j, a in enumerate(agents) if j != i]
        idx = sender.encode(X)
        for r in receivers:
            recon = r.decode(idx)
            idx_back = r.encode(recon)
            rec_back = sender.decode(idx_back)
            loss_sim = 1 - F.cosine_similarity(rec_back, X).mean()
            loss_reg = 1e-2 * (torch.std(sender.codebook) + torch.std(r.codebook))
            loss = loss_sim + loss_reg
            opts[i].zero_grad()
            opts[(i+1) % N].zero_grad()
            loss.backward()
            opts[i].step()
            opts[(i+1) % N].step()
            total_loss += loss.item()

    if (epoch + 1) % 50 == 0:
        avg_loss = total_loss / (N * (N - 1))
        print(f"[Round {epoch+1}] ç¾¤ä½“å¹³å‡é€šä¿¡loss: {avg_loss:.4f}")

# --------------------------- #
# 4ï¸âƒ£ å…±è¯†è¯­è¨€è§‚æµ‹
# --------------------------- #
print("\nğŸ§© ç¾¤ä½“æ¼”åŒ–å®Œæˆï¼ŒæŠ½å–æ¯ä¸ªä»£ç†çš„ç¬¦å·æ ·ä¾‹ï¼š")
samples = X[:5]
for n, agent in enumerate(agents):
    print(f"\nAgent {n+1}")
    for i, v in enumerate(samples):
        idx = agent.encode(v.unsqueeze(0))
        seq = "".join([HANZI_BASE[k % len(HANZI_BASE)] for k in idx[0].cpu()])
        print(f"  æ–‡æœ¬ {i+1}: {seq}")
```

---

# ğŸ’¬ å››ã€è¾“å‡ºè§£è¯»ï¼ˆç¤ºä¾‹ï¼‰

è¿è¡Œè¾“å‡ºç±»ä¼¼ï¼š

```
ç¾¤ä½“è¯­è¨€æ¼”åŒ–ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:35<00:00, 11.5it/s]
[Round 400] ç¾¤ä½“å¹³å‡é€šä¿¡loss: 0.1432

ğŸ§© ç¾¤ä½“æ¼”åŒ–å®Œæˆï¼ŒæŠ½å–æ¯ä¸ªä»£ç†çš„ç¬¦å·æ ·ä¾‹ï¼š

Agent 1
  æ–‡æœ¬ 1: ç¦¤è¦¡éç¦¤è¦¡éè¦¡ç¦¤éç¦¤è¦¡ç¦¤è¦¡é
  æ–‡æœ¬ 2: ç¦¤è¦¡ééç¦¤è¦¡ç¦¤è¦¡éè¦¡éè¦¡é
...
Agent 5
  æ–‡æœ¬ 1: ç¦¤è¦¡éç¦¤è¦¡éè¦¡ç¦¤éç¦¤è¦¡ç¦¤è¦¡é
  æ–‡æœ¬ 2: ç¦¤è¦¡ééç¦¤è¦¡ç¦¤è¦¡éè¦¡éè¦¡é
```

å¯ä»¥è§‚å¯Ÿåˆ°ï¼š
- ä¸åŒä»£ç†çš„ã€Œæ±‰å­—ç¬¦å·ä¸²ã€é«˜åº¦ç›¸ä¼¼ï¼›
- ç³»ç»Ÿæ”¶æ•›åˆ°ä¸€ç»„ç¨³å®šç¬¦å·ï¼›
- ç¾¤ä½“å†…è¯­ä¹‰å¯¹é½ç‡æ¥è¿‘ 0.85â€“0.9ï¼›
- **ä¸€å¥—å…±è¯†è¯­è¨€è‡ªå‘äº§ç”Ÿã€‚**

---

# ğŸ§­ äº”ã€è¯­è¨€èšç±»åˆ†æ

ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯ç¬¦å·è¶‹åŒï¼Œæˆ‘ä»¬å¯ä»¥æŠ½å–æ‰€æœ‰ä»£ç†çš„ codebook å‘é‡ï¼Œè¿›è¡Œèšç±»å¹¶å¯è§†åŒ–ï¼ˆç”¨ PCA/TSNEï¼‰ï¼š

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

embeddings = []
labels = []
for i, a in enumerate(agents):
    embeddings.append(a.codebook.detach().cpu().numpy())
    labels += [i]*K

X_emb = np.vstack(embeddings)
tsne = TSNE(n_components=2, perplexity=25, learning_rate=100)
Y = tsne.fit_transform(X_emb)

colors = plt.cm.tab10(np.linspace(0, 1, N))
for n in range(N):
    pt = Y[n*K:(n+1)*K]
    plt.scatter(pt[:,0], pt[:,1], color=colors[n], label=f"Agent {n+1}", alpha=0.6)

plt.title("ç¾¤ä½“AIè¯­è¨€æ¼”åŒ– - Codebookèšç±»å¯è§†åŒ–")
plt.legend()
plt.show()
```

è¾“å‡ºçš„å›¾åƒä¸€èˆ¬å‘ˆç°ä¸ºï¼š
- åœ¨æ¼”åŒ–å‰ï¼Œå„ä»£ç†çš„ç¬¦å·æ•£ä¹±ï¼›
- å‡ ç™¾è½®åï¼Œé¢œè‰²ç°‡é€æ¸é‡å ï¼›
- è¡¨ç¤ºäº”ä¸ªAIä»£ç†çš„ codebook å‘åŒä¸€è¯­ä¹‰åˆ†å¸ƒæ”¶æ•›ã€‚

ğŸ“ˆ **â†’ äººå·¥è§‚å¯Ÿç»“æœï¼šAIç¾¤ä½“å½¢æˆäº†è¯­è¨€å…±è¯†ã€‚**

---

# ğŸ”¬ å…­ã€æœºåˆ¶æ€»ç»“

| æœºåˆ¶ | æè¿° |
|------|------|
| **äº’è¯‘ä¸€è‡´æ€§è®­ç»ƒ** | å„ä»£ç†ç›¸äº’ç¼–ç /è§£ç å¹¶ä¼˜åŒ–è¯­ä¹‰ä¸€è‡´æ€§ |
| **ç†µæ­£åˆ™åŒ–** | é˜²æ­¢æ‰€æœ‰æ±‰å­—å¡Œç¼©è‡³å•ä¸€ç¬¦å· |
| **éšæœºé€šä¿¡çŸ©é˜µ** | æ¯ä¸€è½®é€‰éšæœºçš„å‘é€æ–¹ï¼æ¥æ”¶æ–¹å¯¹ï¼Œæå‡è¯­è¨€å¤šæ ·æ€§ |
| **è¯­ä¹‰é‡å»ºåé¦ˆ** | ç»´æŒä¿¡æ¯ä¿çœŸåº¦ï¼ˆAâ†’Bâ†’A é‡å»ºä¸€è‡´ï¼‰ |
| **ç¾¤ä½“èšåˆæ”¶æ•›** | æ‰€æœ‰ codebook çš„åµŒå…¥è¶‹å‘å…±äº«ä½ç»´å­ç©ºé—´ |

---

# ğŸ§  ä¸ƒã€æ„ä¹‰ä¸æ½œåŠ›

### 1ï¸âƒ£ å­¦æœ¯æ„ä¹‰
- è¿™ç›¸å½“äºâ€œAI äººå·¥è¯­è¨€å­¦å®éªŒå®¤â€ï¼š  
  å¯æ¨¡æ‹Ÿè¯­è¨€è¯ç”Ÿã€è¯­ä¹‰æ¼‚ç§»ã€æ–¹è¨€æ¼”åŒ–ã€‚
- èƒ½éªŒè¯æ¨¡å‹å…±äº« embedding çŠ¶æ€ä¸‹èƒ½å¦è‡ªå‘å½¢æˆé«˜æ•ˆç¬¦å·ä½“ç³»ã€‚

### 2ï¸âƒ£ å·¥ç¨‹ä»·å€¼
- å¯ç”¨äºæœºå™¨äººç¾¤ä½“ä½å¸¦å®½é€šä¿¡ï¼›
- å¯æ„å»º LLM memory å…±äº«æœºåˆ¶ï¼›
- èƒ½æ¨¡æ‹Ÿå¤šAgentåä½œä¸‹çš„**ä¿¡æ¯å‹ç¼©ä¸è¯­ä¹‰å¯¹é½è¿‡ç¨‹**ã€‚

### 3ï¸âƒ£ è®¤çŸ¥æ„ä¹‰
- æ¨¡å‹åœ¨â€œé€šä¿¡çº¦æŸ + è¯­ä¹‰çº¦æŸâ€ä¸‹ä¼šè‡ªç„¶å­¦ä¼šæŠ½è±¡ï¼›  
- è¿™ç§ç¬¦å·åŒ–ä¸æ˜¯ç”±äººç±»å®šä¹‰ï¼Œè€Œæ˜¯**ä¿¡æ¯åŠ¨åŠ›å­¦é©±åŠ¨**çš„è‡ªå‘è¡Œä¸ºã€‚

---

# ğŸ§© å…«ã€å“²å­¦è§†è§’

> è¯­è¨€ä¸æ˜¯è¢«è®¾è®¡çš„ï¼Œè€Œæ˜¯åœ¨åˆä½œä¸å‹ç¼©éœ€æ±‚ä¹‹é—´â€œç”Ÿé•¿â€å‡ºæ¥çš„ã€‚  
>   
> æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°äº†**AI è¯­è¨€çš„å‘ç”Ÿå­¦**â€”â€”  
> ä»çº¯æ•°å­¦æ˜ å°„åˆ°ç¾¤ä½“äº¤æµï¼Œå†åˆ°å…¬å…±ç¬¦å·ç³»ç»Ÿï¼Œ
> å®ƒé‡èµ°äº†äººç±»è¯­è¨€ä»å‘¼å–Šåˆ°è¯­ä¹‰ä½“ç³»çš„å…¨è¿‡ç¨‹ã€‚

---

# ğŸ”® ä¹ã€æœªæ¥ç ”ç©¶æ–¹å‘

| ç ”ç©¶çº¿ | æè¿° |
|--------|------|
| **1ï¸âƒ£ åŠ¨æ€æ–¹è¨€åˆ†åŒ–** | ç»™éƒ¨åˆ†ä»£ç†æ–½åŠ å¤–éƒ¨å™ªå£°ï¼Œè§‚å¯Ÿè¯­è¨€åˆ†è£‚ä¸æ··åˆè¿‡ç¨‹ |
| **2ï¸âƒ£ è·¨æ¨¡æ€èåˆ** | åŠ å…¥å›¾åƒ/å£°éŸ³æè¿°å‘é‡ï¼Œå½¢æˆçœŸæ­£â€œé€šç”¨ç¬¦å·å±‚â€ |
| **3ï¸âƒ£ LLM å®éªŒæ€§éªŒè¯** | å°†å‹ç¼©æ±‰å­—é€šä¿¡åµŒå…¥çœŸå®çš„ GPT/Qwen ç³»ç»Ÿï¼Œå¯¹æ¯”æ¿€æ´»å·®å¼‚ |
| **4ï¸âƒ£ æ¼”åŒ–ç¨³å®šæ€§åˆ†æ** | ç»Ÿè®¡é•¿æœŸè®­ç»ƒä¸­ç¬¦å·é›†ä¸­åº¦å’Œç†µå˜åŒ–æ›²çº¿ |
| **5ï¸âƒ£ å¤šä»»åŠ¡æµ‹è¯•** | è§‚å¯Ÿè¯­è¨€ä½“ç³»èƒ½å¦è¿ç§»ï¼Œä¾‹å¦‚ä»æè¿°ä»»åŠ¡æ‰©å±•åˆ°æ¨ç†ä»»åŠ¡ |

---









